{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f71d4825",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n",
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dss/dsshome1/03/ge87wod2/morphological-inflection/preprocessing/hierarchical_schema/src\n",
      "hierarchical_features.ipynb\n"
     ]
    }
   ],
   "source": [
    "%cd ~/morphological-inflection/preprocessing/hierarchical_schema/src/\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "804e3956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "more tables than 500 needed for lang: ote  798 744 6426\n",
      "ote needed 1000 tables.\n",
      "more tables than 500 needed for lang: pol  851 772 7001\n",
      "pol needed 1000 tables.\n",
      "number of lemmas in csb testfile:  37\n"
     ]
    }
   ],
   "source": [
    "# split and sample from the paradigms according to the SIGMORPHONâ€“UniMorph 2023 Shared Task 0 Paper\n",
    "def get_lemma(line):\n",
    "    lemma = line.split(\"\\t\")[0]\n",
    "    return lemma\n",
    "\n",
    "from random import sample,shuffle\n",
    "from math import floor\n",
    "from itertools import groupby\n",
    "path_data = \"../data/\"\n",
    "LANGS = [\"ces\",\"dsb\",\"slk\"]\n",
    "LANGS = [\"ote\",\"pol\",\"csb\"]\n",
    "\n",
    "for lang in LANGS:\n",
    "    with open (path_data + lang,\"r\") as data_file, open (path_data + lang + \".trn\",\"w\") as train_file, open (path_data + lang + \".dev\",\"w\") as dev_file,open (path_data + lang + \".tst\",\"w\") as test_file:\n",
    "        data_lines = [line.strip() for line in data_file.readlines()]\n",
    "        tables = [(k, list(g)) for k, g in groupby(data_lines, get_lemma) if k != \"\"]\n",
    "        if lang != \"csb\":\n",
    "            for n in (500,1000,2000,3000):\n",
    "                if len (tables) < n: n = len (tables)\n",
    "                sampled_tables = sample(tables,n)\n",
    "                size = len(sampled_tables)\n",
    "                datasets = [ sampled_tables[:floor(0.8*size)], sampled_tables[floor(0.8*size):floor(0.9*size)], sampled_tables[floor(0.9*size):] ]\n",
    "\n",
    "                train_forms = [form for lemma,table in datasets[0] for form in table]\n",
    "                dev_forms = [form for lemma,table in datasets[1] for form in table]\n",
    "                test_forms = [form for lemma,table in datasets[2] for form in table]\n",
    "\n",
    "                if len(dev_forms) < 1000 or len(test_forms) < 1000 or len(train_forms) < 10000:\n",
    "                    print(f\"more tables than {n} needed for lang: {lang} \",len(dev_forms),len(test_forms) ,len(train_forms))\n",
    "                else:\n",
    "                    print(f\"{lang} needed {n} tables.\")\n",
    "                    break\n",
    "                    \n",
    "\n",
    "            # sampling and retaining order of the forms\n",
    "            indices_train = sorted(sample(range(len(train_forms)), 10000))\n",
    "            indices_dev =  sorted(sample(range(len(dev_forms)), 1000))\n",
    "            indices_test = sorted( sample(range(len(test_forms)), 1000))\n",
    "\n",
    "            train_samples = [train_forms[i] for i in indices_train]\n",
    "            dev_samples = [dev_forms[i] for i in indices_dev]\n",
    "            test_samples = [test_forms[i] for i in indices_test]\n",
    "            \n",
    "           \n",
    "            train_txt = \"\\n\".join(train_samples) + \"\\n\"\n",
    "            dev_txt = \"\\n\".join(dev_samples) + \"\\n\"\n",
    "            test_txt = \"\\n\".join(test_samples) + \"\\n\"\n",
    "\n",
    "            train_file.write(train_txt)\n",
    "            dev_file.write(dev_txt)\n",
    "            test_file.write(test_txt)\n",
    "        elif lang == \"csb\":\n",
    "            # shuffle lemmas\n",
    "            shuffle(tables) # lemmas should be randomly ordered\n",
    "            test_forms = [form for lemma,table in tables for form in table]\n",
    "            length = len([(k, list(g)) for k, g in groupby(test_forms, get_lemma)])\n",
    "            print(f\"number of lemmas in csb testfile: \",length)\n",
    "            test_txt = \"\\n\".join(test_forms) + \"\\n\"\n",
    "            test_file.write(test_txt)\n",
    "\n",
    "\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4f3f666b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='../data/ote.trn' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='../data/ote.dev' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='../data/ote.tst' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='../data/pol.trn' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='../data/pol.dev' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='../data/pol.tst' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='../data/csb.tst' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='../data/ces.trn' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='../data/ces.dev' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='../data/ces.tst' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='../data/dsb.trn' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='../data/dsb.dev' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='../data/dsb.tst' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='../data/slk.trn' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='../data/slk.dev' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='../data/slk.tst' mode='r' encoding='UTF-8'>\n",
      "['V', 'IPFV', 'SG', '1', 'PRS', '2', '3', 'PST', 'PFV', 'PRF', 'IRR', 'ADJ', 'FEM', 'ACC', 'INS', 'NOM', 'PL', 'NEUT', 'MASC', 'ANIM', 'GEN', 'HUM', 'DAT', 'ESS', 'N', 'VOC', 'COND', 'INAN', 'FUT', 'IMP', 'NFIN', 'V.PTCP', 'ACT', 'V.MSDR', 'PASS', 'V.CVB', 'ADV', 'CMPR', 'SPRL', 'DU', 'IND', 'NEG']\n"
     ]
    }
   ],
   "source": [
    "# unique tags    \n",
    "tags = []\n",
    "path_data = \"../data/\"\n",
    "LANGS = [\"ote\",\"pol\",\"csb\",\"ces\",\"dsb\",\"slk\"]\n",
    "datasets = [\"trn\",\"dev\",\"tst\"]\n",
    "for lang in LANGS:\n",
    "    if lang == \"csb\": datasets = [\"tst\"]\n",
    "    else: datasets = [\"trn\",\"dev\",\"tst\"]\n",
    "    for dataset in datasets:\n",
    "        with open (path_data + lang + \".\"+dataset,\"r\") as data_file:\n",
    "            print(data_file)\n",
    "            data_lines = data_file.readlines()\n",
    "            for line in data_lines:\n",
    "                line = line.strip()\n",
    "                if line == \"\": continue\n",
    "                lemma, target, features = line.split(\"\\t\",2)\n",
    "                features = features.replace(\"\\t\",\";\") # features are in last or two last columns\n",
    "                features = features.split(\";\")\n",
    "                for feature in features: \n",
    "                    if feature not in tags: \n",
    "                        tags += [feature]   \n",
    "print(tags)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3306d0cb-1cda-4580-a09c-99ffb9f0be17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify each feature, sort and rearrange into hierarchical schema\n",
    "features_in_ote_pol_csb_new = ['V', 'IPFV', 'SG', '1', 'PRS', 'PST', '2', '3', 'PFV', 'PRF', 'IRR', 'MASC', 'N', 'ESS', 'PL', 'INS', 'NOM', 'ACC', 'DAT', 'GEN', 'VOC', 'ADJ', 'FEM', 'NEUT', 'HUM', 'INAN', 'ANIM', 'FUT', 'IMP', 'V.PTCP', 'ACT', 'V.CVB', 'COND', 'PASS', 'V.MSDR', 'NFIN']\n",
    "features = ['V', 'IPFV', 'SG', '2', 'PRS', '3', 'PST', 'PFV', 'PRF', '1', 'IRR', 'N', 'ESS', 'PL', 'INS', 'NOM', 'GEN', 'DAT', 'ACC', 'VOC', 'ADJ', 'FEM', 'NEUT', 'MASC', 'ANIM', 'HUM', 'INAN', 'IMP', 'FUT', 'V.MSDR', 'V.PTCP', 'PASS', 'COND', 'V.CVB', 'ACT', 'NFIN', 'ADV', 'CMPR', 'SPRL', 'DU', 'IND', 'NEG']\n",
    "features_new = ['V', 'IPFV', 'SG', '1', 'PRS', '2', '3', 'PST', 'PFV', 'PRF', 'IRR', 'ADJ', 'FEM', 'ACC', 'INS', 'NOM', 'PL', 'NEUT', 'MASC', 'ANIM', 'GEN', 'HUM', 'DAT', 'ESS', 'N', 'VOC', 'COND', 'INAN', 'FUT', 'IMP', 'NFIN', 'V.PTCP', 'ACT', 'V.MSDR', 'PASS', 'V.CVB', 'ADV', 'CMPR', 'SPRL', 'DU', 'IND', 'NEG']\n",
    "for f in features_new:\n",
    "    if f not in features:\n",
    "        print(\"add feature \", f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "05950671-b843-40a9-a22d-6e82c08b992d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension per feature: {'V': 'Parts of Speech', 'IPFV': 'Aspect', 'SG': 'Number', '2': 'Person', 'PRS': 'Tense', '3': 'Person', 'PST': 'Tense', 'PFV': 'Aspect', 'PRF': 'Aspect', '1': 'Person', 'IRR': 'Mood', 'N': 'Parts of Speech', 'ESS': 'Case', 'PL': 'Number', 'INS': 'Case', 'NOM': 'Case', 'GEN': 'Case', 'DAT': 'Case', 'ACC': 'Case', 'VOC': 'Case', 'ADJ': 'Parts of Speech', 'FEM': 'Gender+', 'NEUT': 'Gender+', 'MASC': 'Gender+', 'ANIM': 'Animacy', 'HUM': 'Animacy', 'INAN': 'Animacy', 'IMP': 'Mood', 'FUT': 'Tense', 'V.MSDR': 'Parts of Speech', 'V.PTCP': 'Parts of Speech', 'PASS': 'Voice', 'COND': 'Mood', 'V.CVB': 'Parts of Speech', 'ACT': 'Voice', 'NFIN': 'Finiteness', 'ADV': 'Parts of Speech', 'CMPR': 'Comparison', 'SPRL': 'Comparison', 'DU': 'Number', 'IND': 'Mood', 'NEG': 'Polarity'}\n",
      "features per dimension: {'Animacy': ['ANIM', 'HUM', 'INAN'], 'Aspect': ['IPFV', 'PFV', 'PRF'], 'Case': ['ACC', 'DAT', 'ESS', 'GEN', 'INS', 'NOM', 'VOC'], 'Comparison': ['CMPR', 'SPRL'], 'Finiteness': ['NFIN'], 'Gender+': ['FEM', 'MASC', 'NEUT'], 'Mood': ['COND', 'IMP', 'IND', 'IRR'], 'Number': ['DU', 'PL', 'SG'], 'Parts of Speech': ['ADJ', 'ADV', 'N', 'V', 'V.CVB', 'V.MSDR', 'V.PTCP'], 'Person': ['1', '2', '3'], 'Polarity': ['NEG'], 'Tense': ['FUT', 'PRS', 'PST'], 'Voice': ['ACT', 'PASS']}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# soruce https://aclanthology.org/P15-2111.pdf\n",
    "\n",
    "features_per_dimension = {\n",
    "    \"Aktionsart\": [\"ACCMP\", \"ACH\", \"ACTY\", \"ATEL\", \"DUR\", \"DYN\", \"PCT\", \"SEMEL\", \"STAT\", \"TEL\"],\n",
    "    'Animacy': [\"ANIM\", \"HUM\", \"INAN\", \"NHUM\"],\n",
    "    'Aspect': [\"HAB\", \"IPFV\", \"ITER\", \"PFV\", \"PRF\", \"PROG\", \"PROSP\"],\n",
    "    \"Case\": [\"ABL\", \"ABS\", \"ACC\", \"ALL\", \"ANTE\", \"APPRX\", \"APUD\", \"AT\", \"AVR\", \"BEN\", \"CIRC\", \"COM\", \"COMPV\", \"DAT\", \"EQU\",\n",
    "\"ERG\", \"ESS\", \"FRML\", \"GEN\", \"INS\", \"IN\", \"INTER\", \"NOM\", \"NOMS\", \"ON\", \"ONHR\", \"ONVR\", \"POST\", \"PRIV\", \"PROL\",\n",
    "\"PROPR\", \"PROX\", \"PRP\", \"PRT\", \"REM\", \"SUB\", \"TERM\", \"VERS\", \"VOC\"],\n",
    "\"Comparison\": [\"AB\", \"CMPR\", \"EQT\", \"RL\", \"SPRL\"],\n",
    "\"Definiteness\": [\"DEF\", \"INDEF\", \"NSPEC\", \"SPEC\"],\n",
    "\"Deixis\": [\"ABV\", \"BEL\", \"DIST\", \"EVEN\", \"MED\", \"NVIS\", \"PROX\", \"REF1\", \"REF2\", \"REM\", \"VIS\" ],\n",
    "\"Evidentiality\": [\"ASSUM\", \"AUD\", \"DRCT\", \"FH\", \"HRSY\", \"INFER\", \"NFH\" , \"NVSEN\", \"QUOT\", \"RPRT\", \"SEN\"],\n",
    "\"Finiteness\": [\"FIN\", \"NFIN\" ],\n",
    "\"Gender+\" :[\"BANTU1-23\", \"FEM\", \"MASC\", \"NAKH1-8\", \"NEUT\" ],\n",
    "\"Info. Structure\": [\"FOC\", \"TOP\" ],\n",
    "\"Interrogativity\": [\"DECL\", \"INT\" ],\n",
    "\"Mood\": [\"ADM\", \"AUNPRP\", \"AUPRP\", \"COND\", \"DEB\", \"IMP\", \"IND\", \"INTEN\", \"IRR\", \"LKLY\", \"OBLIG\", \"OPT\",\n",
    "\"PERM\", \"POT\", \"PURP\", \"REAL\", \"SBJV\", \"SIM\"],\n",
    "\"Number\": [\"DU\", \"GPAUC\", \"GRPL\", \"INVN\", \"PAUC\", \"PL\", \"SG\", \"TRI\"],\n",
    "\"Parts of Speech\": [\"ADJ\", \"ADP\", \"ADV\", \"ART\", \"AUX\", \"CLF\", \"COMP\", \"CONJ\", \"DET\", \"INTJ\", \"N\", \"NUM\", \"PART\", \"PRO\",\n",
    "\"V\", \"V.CVB\", \"V.MSDR\", \"V.PTCP\"],\n",
    "\"Person\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"EXCL\", \"INCL\", \"OBV\", \"PRX\"],\n",
    "\"Polarity\":  [\"NEG\", \"POS\"],\n",
    "\"Politeness\": [\"AVOID\", \"COL\", \"FOREG\", \"FORM\", \"FORM.ELEV\", \"FORM.HUMB\", \"HIGH\", \"HIGH.ELEV\",\n",
    "\"HIGH.SUPR\", \"INFM\", \"LIT\", \"LOW\", \"POL\"],\n",
    "\"Possession\": [\"ALN\", \"NALN\", \"PSSD\", \"PSSPNO+\"],\n",
    "\"Switch-Reference\": [\"CN-R-MN+\", \"DS\", \"DSADV\", \"LOG\", \"OR\", \"SEQMA\", \"SIMMA\", \"SS\", \"SSADV\"],\n",
    "\"Tense\": [\"1DAY\", \"FUT\", \"HOD\", \"IMMED\", \"PRS\", \"PST\", \"RCT\", \"RMT\" ],\n",
    "\"Valency\": [\"DITR\", \"IMPRS\", \"INTR\", \"TR\" ],\n",
    "\"Voice\": [\"ACFOC\", \"ACT\", \"AGFOC\", \"ANTIP\", \"APPL\", \"BFOC\", \"CAUS\", \"CFOC\", \"DIR\", \"IFOC\", \"INV\", \"LFOC\",\n",
    "\"MID\", \"PASS\", \"PFOC\", \"RECP\", \"REFL\"],\n",
    "}\n",
    "dimension_per_feature = {}\n",
    "for f in features:\n",
    "    key = [k for k,values in features_per_dimension.items()  if f in values][0]\n",
    "    dimension_per_feature [f] = key\n",
    "print(\"dimension per feature:\",dimension_per_feature)\n",
    "\n",
    "def intersect (list1,list2):\n",
    "    return [l for l in list1 if l in list2]\n",
    "    \n",
    "features_per_dimension = [(d,intersect(f,features)) for d, f in features_per_dimension.items()]\n",
    "features_per_dimension = [(d,f) for d,f in features_per_dimension if f!=[]]\n",
    "features_per_dimension = dict(features_per_dimension) \n",
    "\n",
    "\n",
    "print(\"features per dimension:\",features_per_dimension)\n",
    "features_per_dimension ={\n",
    "    'Animacy': ['ANIM', 'HUM', 'INAN'], \n",
    "    'Aspect': ['IPFV', 'PFV', 'PRF'], \n",
    "    'Case': ['ACC', 'DAT', 'ESS', 'GEN', 'INS', 'NOM', 'VOC'], \n",
    "    'Comparison': ['CMPR', 'SPRL'], \n",
    "    'Finiteness': ['NFIN'], \n",
    "    'Gender+': ['FEM', 'MASC', 'NEUT'], 'Mood': ['COND', 'IMP', 'IND', 'IRR'], \n",
    "    'Number': ['DU', 'PL', 'SG'], \n",
    "    'Parts of Speech': ['ADJ', 'ADV', 'N', 'V', 'V.CVB', 'V.MSDR', 'V.PTCP'], \n",
    "    'Person': ['1', '2', '3'], 'Polarity': ['NEG'], \n",
    "    'Tense': ['FUT', 'PRS', 'PST'], \n",
    "    'Voice': ['ACT', 'PASS'] \n",
    "}\n",
    "\n",
    "low_order_hierarchy = {\n",
    "    'Person': 1,\n",
    "    'Number': 2,\n",
    "    'Gender': 3,\n",
    "    'Animacy': 4\n",
    "}\n",
    "\n",
    "low_hierarchy_of_dimension = {\n",
    "    'Parts of Speech': False, #high\n",
    "    'Aspect': False,# high\n",
    "    'Number': True ,# low\n",
    "    'Person': True,# low\n",
    "    'Mood':  False,# high\n",
    "    'Case':  False,# high, (in front of low)\n",
    "    'Gender+':  True,# low\n",
    "    'Tense':  False,# high\n",
    "    'Animacy': True,# low\n",
    "    'Voice':  False,# high\n",
    "    'Finiteness': False, # high\n",
    "    'Comparison': False, # high\n",
    "    'Polarity': False, # high\n",
    "\n",
    "}\n",
    "\n",
    "hierarchy_per_dimension ={\n",
    "    'Animacy': 6,\n",
    "    'Aspect': 1,\n",
    "    'Case': 2,\n",
    "    'Comparison': 1, \n",
    "    'Finiteness': 1, \n",
    "    'Gender+': 5,\n",
    "    'Mood': 1, \n",
    "    'Number': 4, \n",
    "    'Parts of Speech': 0, # first\n",
    "    'Person': 3,\n",
    "    'Polarity': 1, \n",
    "    'Tense': 1, \n",
    "    'Voice':1\n",
    "}\n",
    "\n",
    "def hierarchical_schema (features):\n",
    "    split_features = features.split(\";\")\n",
    "    POS = split_features[0]\n",
    "    sorted_features = sorted(split_features, key=lambda feature: hierarchy_per_dimension[dimension_per_feature[feature]])\n",
    "    low_hierarchy_features  = filter  (lambda feature:      low_hierarchy_of_dimension[dimension_per_feature[feature]], sorted_features)\n",
    "    high_hierarchy_features = filter  (lambda feature: not  low_hierarchy_of_dimension[dimension_per_feature[feature]], sorted_features)\n",
    "    case = [f for f in sorted_features if dimension_per_feature[f] == \"Case\"]\n",
    "    if POS in ['V', 'V.CVB', 'V.MSDR', 'V.PTCP']:\n",
    "        if case == [] or low_hierarchy_features == []:\n",
    "            return \";\".join(sorted_features)\n",
    "        else:\n",
    "            return \";\".join(high_hierarchy_features) + \";NOM(\" + \",\".join(low_hierarchy_features) +\")\"\n",
    "    elif POS in ['ADJ', 'ADV', 'N']:\n",
    "        if case == [] or low_hierarchy_features == []:\n",
    "            return \";\".join(sorted_features)\n",
    "        else:\n",
    "            return \";\".join(high_hierarchy_features) + \"(\" + \",\".join(low_hierarchy_features) + \")\"\n",
    "\n",
    "def process_line(x):\n",
    "    x = x.strip()\n",
    "    lemma, target, features = x.split(\"\\t\",2)\n",
    "    features = features.replace(\"\\t\",\";\") # features are in last or two last columns\n",
    "    processed_features = hierarchical_schema(features)\n",
    "    line = lemma + \"\\t\" + processed_features + \"\\t\" + target\n",
    "    return line\n",
    "\n",
    "path_data = \"../data/\"\n",
    "OUT_DIR = \"../../../2023InflectionST/part1/data/\"\n",
    "LANGS = [\"dsb\",\"slk\",\"ces\",\"ote\",\"pol\",\"csb\"]\n",
    "datasets = [\"trn\",\"dev\",\"tst\"]\n",
    "for lang in LANGS:\n",
    "    if lang == \"csb\": datasets = [\"tst\"]\n",
    "    else: datasets = [\"trn\",\"dev\",\"tst\"]\n",
    "    for dataset in datasets:\n",
    "        with open (path_data + lang + \".\"+dataset,\"r\") as data_file, open (OUT_DIR + lang + \".\" + dataset,\"w\") as output_file:\n",
    "            data_lines = data_file.readlines()\n",
    "            output_file.write(\"\\n\".join([process_line(line) for line in data_lines]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51866382",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# soruce https://aclanthology.org/P15-2111.pdf\n",
    "# features_per_dimension = {'Parts_of_Speech': ['V', 'N', 'ADJ', 'V.PTCP', 'V.MSDR', 'V.CVB'], # high\n",
    "# 'Aspect': ['IPFV', 'PFV', 'PRF'], # high\n",
    "# 'Number': ['SG', 'PL'], # low\n",
    "# 'Person': ['2', '3', '1'],  # low\n",
    "# 'Mood': ['IRR', 'COND', 'IMP'], # high\n",
    "# 'Case': ['ESS', 'GEN', 'NOM', 'INS', 'DAT', 'VOC', 'ACC'], # high, (in front of low)\n",
    "# 'Gender': ['FEM', 'MASC', 'NEUT'],  # low\n",
    "# 'Tense': ['FUT', 'PRS', 'PST'], # high\n",
    "# 'Animacy': ['HUM', 'ANIM', 'INAN'], # low\n",
    "# 'Voice': ['PASS', 'ACT'], # high\n",
    "# 'Finiteness': ['NFIN']} # high\n",
    "# dimension_per_feature = {\n",
    "#     'V': 'Parts_of_Speech', 'IPFV': 'Aspect', 'SG': 'Number', 'PFV': 'Aspect', 'PRF': 'Aspect', '2': 'Person', '3': 'Person', 'IRR': 'Mood', '1': 'Person', 'N': 'Parts_of_Speech', 'ESS': 'Case', 'PL': 'Number', 'GEN': 'Case', 'NOM': 'Case', 'ADJ': 'Parts_of_Speech', 'INS': 'Case', 'DAT': 'Case', 'VOC': 'Case', 'FEM': 'Gender', 'FUT': 'Tense', 'PRS': 'Tense', 'PST': 'Tense', 'COND': 'Mood', 'IMP': 'Mood', 'ACC': 'Case', 'MASC': 'Gender', 'HUM': 'Animacy', 'NEUT': 'Gender', 'V.PTCP': 'Parts_of_Speech', 'PASS': 'Voice', 'ACT': 'Voice', 'ANIM': 'Animacy', 'V.MSDR': 'Parts_of_Speech', 'INAN': 'Animacy', 'V.CVB': 'Parts_of_Speech', 'NFIN': 'Finiteness'}\n",
    "# low_hierarchy_of_dimension = {\n",
    "#     'Parts_of_Speech': False, #high\n",
    "#     'Aspect': False,# high\n",
    "#     'Number': True ,# low\n",
    "#     'Person': True,# low\n",
    "#     'Mood':  False,# high\n",
    "#     'Case':  False,# high, (in front of low)\n",
    "#     'Gender':  True,# low\n",
    "#     'Tense':  False,# high\n",
    "#     'Animacy': True,# low\n",
    "#     'Voice':  False,# high\n",
    "#     'Finiteness': False, # high\n",
    "# }\n",
    "\n",
    "# low_order_priority = {\n",
    "#     'Person': 1,\n",
    "#     'Number': 2,\n",
    "#     'Gender': 3,\n",
    "#     'Animacy': 4\n",
    "# }\n",
    "\n",
    "# order_priority = {\n",
    "#     'Parts_of_Speech': 0, #high\n",
    "#     'Aspect': 1,# high\n",
    "#     'Number': 4 ,# low\n",
    "#     'Person': 3,# low\n",
    "#     'Mood':  1,# high\n",
    "#     'Case':  2,# high, (in front of low)\n",
    "#     'Gender':  5,# low\n",
    "#     'Tense':  1,# high\n",
    "#     'Animacy': 6,# low\n",
    "#     'Voice':  1,# high\n",
    "#     'Finiteness': 1, # high\n",
    "# }\n",
    "\n",
    "def hierarchical_schema (features):\n",
    "    split_features = features.split(\";\")\n",
    "    POS = split_features[0]\n",
    "    sorted_features = sorted(split_features, key=lambda feature: order_priority[dimension_per_feature[feature]])\n",
    "    low_hierarchy_features  = filter  (lambda feature:      low_hierarchy_of_dimension[dimension_per_feature[feature]], sorted_features)\n",
    "    high_hierarchy_features = filter  (lambda feature: not  low_hierarchy_of_dimension[dimension_per_feature[feature]], sorted_features)\n",
    "    if \"V\" in POS:\n",
    "        return \";\".join(high_hierarchy_features) + \";NOM(\" + \",\".join(low_hierarchy_features) +\")\"\n",
    "    elif POS in (\"N\",\"ADJ\"):\n",
    "        return \";\".join(high_hierarchy_features) + \"(\" + \",\".join(low_hierarchy_features) + \")\"\n",
    "\n",
    "def process_line(x):\n",
    "    x = x.strip()\n",
    "    lemma, target, features = x.split(\"\\t\")\n",
    "    processed_features = hierarchical_schema(features)\n",
    "    line = lemma + \"\\t\" + processed_features + \"\\t\" + target\n",
    "    return line\n",
    "\n",
    "path_data = \"data/\"\n",
    "OUT_DIR = \"../../2023InflectionST/part1/data/\"\n",
    "LANGS = [\"ote\",\"pol\",\"csb\"]\n",
    "datasets = [\"trn\",\"dev\",\"tst\"]\n",
    "for lang in LANGS:\n",
    "    if lang == \"csb\": datasets = [\"tst\"]\n",
    "    else: datasets = [\"trn\",\"dev\",\"tst\"]\n",
    "    for dataset in datasets:\n",
    "        with open (path_data + lang + \".\"+dataset,\"r\") as data_file, open (OUT_DIR + lang + \".\" + dataset,\"w\") as output_file:\n",
    "            data_lines = data_file.readlines()\n",
    "            output_file.write(\"\\n\".join([process_line(line) for line in data_lines]) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
