{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f71d4825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dss/dsshome1/03/ge87wod2/morphological-inflection/preprocessing/hierarchical_schema/src\n",
      "hierarchical_features.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:393: UserWarning: using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n",
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ~/morphological-inflection/preprocessing/hierarchical_schema/src/\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "804e3956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "more tables than 500 needed for lang: mkd  812 883 6304\n",
      "mkd needed 1000 tables.\n",
      "more tables than 500 needed for lang: rus  825 781 6744\n",
      "rus needed 1000 tables.\n"
     ]
    }
   ],
   "source": [
    "# split and sample from the paradigms according to the SIGMORPHON–UniMorph 2023 Shared Task 0 Paper\n",
    "def get_lemma(line):\n",
    "    lemma = line.split(\"\\t\")[0]\n",
    "    return lemma\n",
    "\n",
    "from random import sample,shuffle\n",
    "from math import floor\n",
    "from itertools import groupby\n",
    "path_data = \"../data/\"\n",
    "LANGS = [\"ces\",\"dsb\",\"slk\"]\n",
    "LANGS = [\"ote\",\"pol\",\"csb\"]\n",
    "LANGS = [\"mkd\",\"rus\"]\n",
    "\n",
    "for lang in LANGS:\n",
    "    in_path = path_data + lang if lang != \"rus\" else path_data + lang + \".args\"\n",
    "    with open (in_path,\"r\") as data_file, open (path_data + lang + \".trn\",\"w\") as train_file, open (path_data + lang + \".dev\",\"w\") as dev_file,open (path_data + lang + \".tst\",\"w\") as test_file:\n",
    "        data_lines = [line.strip() for line in data_file.readlines()]\n",
    "        tables = [(k, list(g)) for k, g in groupby(data_lines, get_lemma) if k != \"\"]\n",
    "        if lang != \"csb\":\n",
    "            for n in (500,1000,2000,3000):\n",
    "                if len (tables) < n: n = len (tables)\n",
    "                sampled_tables = sample(tables,n)\n",
    "                size = len(sampled_tables)\n",
    "                datasets = [ sampled_tables[:floor(0.8*size)], sampled_tables[floor(0.8*size):floor(0.9*size)], sampled_tables[floor(0.9*size):] ]\n",
    "\n",
    "                train_forms = [form for lemma,table in datasets[0] for form in table]\n",
    "                dev_forms = [form for lemma,table in datasets[1] for form in table]\n",
    "                test_forms = [form for lemma,table in datasets[2] for form in table]\n",
    "\n",
    "                if len(dev_forms) < 1000 or len(test_forms) < 1000 or len(train_forms) < 10000:\n",
    "                    print(f\"more tables than {n} needed for lang: {lang} \",len(dev_forms),len(test_forms) ,len(train_forms))\n",
    "                else:\n",
    "                    print(f\"{lang} needed {n} tables.\")\n",
    "                    break\n",
    "                    \n",
    "\n",
    "            # sampling and retaining order of the forms\n",
    "            indices_train = sorted(sample(range(len(train_forms)), 10000))\n",
    "            indices_dev =  sorted(sample(range(len(dev_forms)), 1000))\n",
    "            indices_test = sorted( sample(range(len(test_forms)), 1000))\n",
    "\n",
    "            train_samples = [train_forms[i] for i in indices_train]\n",
    "            dev_samples = [dev_forms[i] for i in indices_dev]\n",
    "            test_samples = [test_forms[i] for i in indices_test]\n",
    "            \n",
    "           \n",
    "            train_txt = \"\\n\".join(train_samples) + \"\\n\"\n",
    "            dev_txt = \"\\n\".join(dev_samples) + \"\\n\"\n",
    "            test_txt = \"\\n\".join(test_samples) + \"\\n\"\n",
    "\n",
    "            train_file.write(train_txt)\n",
    "            dev_file.write(dev_txt)\n",
    "            test_file.write(test_txt)\n",
    "        elif lang == \"csb\":\n",
    "            # shuffle lemmas\n",
    "            shuffle(tables) # lemmas should be randomly ordered\n",
    "            test_forms = [form for lemma,table in tables for form in table]\n",
    "            length = len([(k, list(g)) for k, g in groupby(test_forms, get_lemma)])\n",
    "            print(f\"number of lemmas in csb testfile: \",length)\n",
    "            test_txt = \"\\n\".join(test_forms) + \"\\n\"\n",
    "            test_file.write(test_txt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b1d90d",
   "metadata": {},
   "source": [
    "# 15k / 20k samples\n",
    "+ add 5k / 10k samples from sme and klr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9cdcab25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sme needed 250 tables.\n",
      "sme needed 500 tables.\n",
      "more tables than 250 needed for lang: klr  2170\n",
      "more tables than 500 needed for lang: klr  4146\n",
      "klr needed 1000 tables.\n",
      "more tables than 500 needed for lang: klr  3969\n",
      "more tables than 1000 needed for lang: klr  8408\n",
      "klr needed 2000 tables.\n"
     ]
    }
   ],
   "source": [
    "# split and sample from the paradigms according to the SIGMORPHON–UniMorph 2023 Shared Task 0 Paper\n",
    "def get_lemma(line):\n",
    "    lemma = line.split(\"\\t\")[0]\n",
    "    return lemma\n",
    "\n",
    "from random import sample,shuffle\n",
    "from math import floor\n",
    "from itertools import groupby\n",
    "path_data = \"../data/\"\n",
    "sigmorphon_path = \"/dss/dsshome1/03/ge87wod2/morphological-inflection/2023InflectionST/part1/data/\"\n",
    "LANGS = [\"sme\",\"klr\"]\n",
    "\n",
    "for lang in LANGS:\n",
    "    for samples in (5000,10000):\n",
    "        out_train = path_data +  lang +\"-add\"+ str(int(samples/1000)) + \"k\" + \".trn\"\n",
    "        in_trn = sigmorphon_path + lang + \".trn\"\n",
    "        in_dev = sigmorphon_path +lang + \".dev\"\n",
    "        in_tst =sigmorphon_path +lang + \".tst\"\n",
    "        with open (path_data + lang,\"r\") as data_file, open (out_train,\"w\") as train_file, open (in_trn,\"r\") as in_trn_file, open (in_dev,\"r\") as in_dev_file, open (in_tst,\"r\") as in_tst_file:\n",
    "            sigmorphon_lines = [line.strip() for line in in_trn_file.readlines() + in_dev_file.readlines() + in_tst_file.readlines() if line != \"\"]\n",
    "            sigmorphon_lemmas = [k for (k, _) in groupby(sigmorphon_lines, get_lemma) if k != \"\"]\n",
    "\n",
    "            data_lines = [line.strip() for line in data_file.readlines() if line!=\"\"]\n",
    "            tables = [(k, list(g)) for k, g in groupby(data_lines, get_lemma) if k != \"\"]\n",
    "\n",
    "            tables = [(lemma,lines) for lemma,lines in tables if lemma not in sigmorphon_lemmas]\n",
    "            sample_ratio = (samples/10000)\n",
    "            for n in (500*sample_ratio,1000*sample_ratio,2000*sample_ratio,3000*sample_ratio): # adjust number of tables according to the ratio to original samples of 10k\n",
    "                n = int(n)\n",
    "                if (len (tables) < n): n = len (tables)\n",
    "                sampled_tables = sample(tables,n)\n",
    "                size = len(sampled_tables)\n",
    "\n",
    "                train_forms = [form for lemma,table in sampled_tables for form in table] # only sample the training set\n",
    "\n",
    "                if ( len(train_forms) < samples):\n",
    "                    print(f\"more tables than {n} needed for lang: {lang} \",len(train_forms))\n",
    "                else:\n",
    "                    print(f\"{lang} needed {n} tables.\")\n",
    "                    break\n",
    "                    \n",
    "\n",
    "            # sampling and retaining order of the forms\n",
    "            indices_train = sorted(sample(range(len(train_forms)), samples))\n",
    "            train_samples = [train_forms[i] for i in indices_train]\n",
    "            train_txt = \"\\n\".join(train_samples) + \"\\n\"\n",
    "            train_file.write(train_txt)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f3f666b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='../data/ote.trn' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='../data/ote.dev' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='../data/ote.tst' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='../data/pol.trn' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='../data/pol.dev' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='../data/pol.tst' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='../data/csb.trn' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='../data/csb.dev' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='../data/csb.tst' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='../data/ces.trn' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='../data/ces.dev' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='../data/ces.tst' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='../data/dsb.trn' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='../data/dsb.dev' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='../data/dsb.tst' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='../data/slk.trn' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='../data/slk.dev' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='../data/slk.tst' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='../data/klr-add5k.trn' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='../data/sme-add5k.trn' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='../data/klr-add10k.trn' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='../data/sme-add10k.trn' mode='r' encoding='UTF-8'>\n",
      "features = ['V', 'IPFV', 'SG', '1', 'PRS', '2', '3', 'PST', 'PFV', 'PRF', 'IRR', 'ADJ', 'FEM', 'ACC', 'INS', 'NOM', 'PL', 'NEUT', 'MASC', 'ANIM', 'GEN', 'HUM', 'DAT', 'ESS', 'N', 'VOC', 'COND', 'INAN', 'FUT', 'IMP', 'NFIN', 'V.PTCP', 'ACT', 'V.MSDR', 'PASS', 'V.CVB', 'ADV', 'CMPR', 'SPRL', 'DU', 'IND', 'NEG', 'POS', 'INCL', 'INTR', 'EXCL', 'ARG1', 'ARGSG', 'TR', 'ARG3', 'ARGDU', 'ARGPL', 'ARGEXCL', 'ARG2', 'ARGINCL', 'REFL', 'POT', 'PRP', 'IN+ALL', 'COM', 'LGSPEC', 'FRML']\n"
     ]
    }
   ],
   "source": [
    "# unique tags    \n",
    "tags = []\n",
    "path_data = \"../data/\"\n",
    "LANGS = [\"ote\",\"pol\",\"csb\",\"ces\",\"dsb\",\"slk\"]\n",
    "LANGS += [\"klr-add5k\",\"sme-add5k\",\"klr-add10k\",\"sme-add10k\"]\n",
    "#LANGS += [\"mkd\"]\n",
    "\n",
    "#datasets = [\"trn\",\"dev\",\"tst\"]\n",
    "datasets = [\"trn\"]\n",
    "\n",
    "for lang in LANGS:\n",
    "    if lang == \"csb\": datasets = [\"tst\"]\n",
    "    if len(lang)>3: datasets=[\"trn\"]\n",
    "    else: datasets = [\"trn\",\"dev\",\"tst\"]\n",
    "    for dataset in datasets:\n",
    "        with open (path_data + lang + \".\"+dataset,\"r\") as data_file:\n",
    "            print(data_file)\n",
    "            data_lines = data_file.readlines()\n",
    "            for line in data_lines:\n",
    "                line = line.strip()\n",
    "                if line == \"\": continue\n",
    "                lemma, target, features = line.split(\"\\t\",2) # feature and target column are swapped\n",
    "                features = features.replace(\"\\t\",\";\") # features are in last or two last columns\n",
    "                features = features.split(\";\")\n",
    "                for feature in features: \n",
    "                    #if \"ARG\" in feature:\n",
    "                        #feature = feature[3:]\n",
    "                    if feature not in tags: \n",
    "                        tags += [feature]   \n",
    "\n",
    "# classify each feature, sort and rearrange into hierarchical schema\n",
    "#features = ['V', 'IPFV', 'SG', '2', 'PRS', '3', 'PST', 'PFV', 'PRF', '1', 'IRR', 'N', 'ESS', 'PL', 'INS', 'NOM', 'GEN', 'DAT', 'ACC', 'VOC', 'ADJ', 'FEM', 'NEUT', 'MASC', 'ANIM', 'HUM', 'INAN', 'IMP', 'FUT', 'V.MSDR', 'V.PTCP', 'PASS', 'COND', 'V.CVB', 'ACT', 'NFIN', 'ADV', 'CMPR', 'SPRL', 'DU', 'IND', 'NEG']\n",
    "features = ['V', 'IPFV', 'SG', '1', 'PRS', '2', '3', 'PST', 'PFV', 'PRF', 'IRR', 'ADJ', 'FEM', 'ACC', 'INS', 'NOM', 'PL', 'NEUT', 'MASC', 'ANIM', 'GEN', 'HUM', 'DAT', 'ESS', 'N', 'VOC', 'COND', 'INAN', 'FUT', 'IMP', 'NFIN', 'V.PTCP', 'ACT', 'V.MSDR', 'PASS', 'V.CVB', 'ADV', 'CMPR', 'SPRL', 'DU', 'IND', 'NEG', 'POS', 'INCL', 'INTR', 'EXCL', 'ARG1', 'ARGSG', 'TR', 'ARG3', 'ARGDU', 'ARGPL', 'ARGEXCL', 'ARG2', 'ARGINCL', 'REFL', 'POT', 'PRP', 'IN+ALL', 'COM', 'LGSPEC', 'FRML']\n",
    "for f in tags:\n",
    "    if f not in features:\n",
    "        print(\"add feature \", f)        \n",
    "features = tags\n",
    "print(\"features =\",features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05950671-b843-40a9-a22d-6e82c08b992d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V\n",
      "IPFV\n",
      "SG\n",
      "1\n",
      "PRS\n",
      "2\n",
      "3\n",
      "PST\n",
      "PFV\n",
      "PRF\n",
      "IRR\n",
      "ADJ\n",
      "FEM\n",
      "ACC\n",
      "INS\n",
      "NOM\n",
      "PL\n",
      "NEUT\n",
      "MASC\n",
      "ANIM\n",
      "GEN\n",
      "HUM\n",
      "DAT\n",
      "ESS\n",
      "N\n",
      "VOC\n",
      "COND\n",
      "INAN\n",
      "FUT\n",
      "IMP\n",
      "NFIN\n",
      "V.PTCP\n",
      "ACT\n",
      "V.MSDR\n",
      "PASS\n",
      "V.CVB\n",
      "ADV\n",
      "CMPR\n",
      "SPRL\n",
      "DU\n",
      "IND\n",
      "NEG\n",
      "POS\n",
      "INCL\n",
      "INTR\n",
      "EXCL\n",
      "ARG1\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(f)\n\u001b[0;32m---> 38\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures_per_dimension\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     39\u001b[0m     dimension_per_feature [f] \u001b[38;5;241m=\u001b[39m key\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdimension per feature:\u001b[39m\u001b[38;5;124m\"\u001b[39m,dimension_per_feature)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "# soruce https://aclanthology.org/P15-2111.pdf,  https://unimorph.github.io/doc/unimorph-schema.pdf\n",
    "\n",
    "features_per_dimension = {\n",
    "    \"Aktionsart\": [\"ACCMP\", \"ACH\", \"ACTY\", \"ATEL\", \"DUR\", \"DYN\", \"PCT\", \"SEMEL\", \"STAT\", \"TEL\"],\n",
    "    'Animacy': [\"ANIM\", \"HUM\", \"INAN\", \"NHUM\"],\n",
    "    'Aspect': [\"HAB\", \"IPFV\", \"ITER\", \"PFV\", \"PRF\", \"PROG\", \"PROSP\"],\n",
    "    \"Case\": [\"ABL\", \"ABS\", \"ACC\", \"ALL\", \"ANTE\", \"APPRX\", \"APUD\", \"AT\", \"AVR\", \"BEN\", \"CIRC\", \"COM\", \"COMPV\", \"DAT\", \"EQU\",\n",
    "\"ERG\", \"ESS\", \"FRML\", \"GEN\", \"INS\", \"IN\",\"IN+ALL\", \"INTER\", \"NOM\", \"NOMS\", \"ON\", \"ONHR\", \"ONVR\", \"POST\", \"PRIV\", \"PROL\",\n",
    "\"PROPR\", \"PROX\", \"PRP\", \"PRT\", \"REM\", \"SUB\", \"TERM\", \"VERS\", \"VOC\"], # case IN+ALL was added since combination of cases https://unimorph.github.io/doc/unimorph-schema.pdf\n",
    "\"Comparison\": [\"AB\", \"CMPR\", \"EQT\", \"RL\", \"SPRL\"],\n",
    "\"Definiteness\": [\"DEF\", \"INDEF\", \"NSPEC\", \"SPEC\"],\n",
    "\"Deixis\": [\"ABV\", \"BEL\", \"DIST\", \"EVEN\", \"MED\", \"NVIS\", \"PROX\", \"REF1\", \"REF2\", \"REM\", \"VIS\" ],\n",
    "\"Evidentiality\": [\"ASSUM\", \"AUD\", \"DRCT\", \"FH\", \"HRSY\", \"INFER\", \"NFH\" , \"NVSEN\", \"QUOT\", \"RPRT\", \"SEN\"],\n",
    "\"Finiteness\": [\"FIN\", \"NFIN\" ],\n",
    "\"Gender+\" :[\"BANTU1-23\", \"FEM\", \"MASC\", \"NAKH1-8\", \"NEUT\" ],\n",
    "\"Info. Structure\": [\"FOC\", \"TOP\" ],\n",
    "\"Interrogativity\": [\"DECL\", \"INT\" ],\n",
    "\"Mood\": [\"ADM\", \"AUNPRP\", \"AUPRP\", \"COND\", \"DEB\", \"IMP\", \"IND\", \"INTEN\", \"IRR\", \"LKLY\", \"OBLIG\", \"OPT\",\n",
    "\"PERM\", \"POT\", \"PURP\", \"REAL\", \"SBJV\", \"SIM\"],\n",
    "\"Number\": [\"DU\", \"GPAUC\", \"GRPL\", \"INVN\", \"PAUC\", \"PL\", \"SG\", \"TRI\"],\n",
    "\"Parts of Speech\": [\"ADJ\", \"ADP\", \"ADV\", \"ART\", \"AUX\", \"CLF\", \"COMP\", \"CONJ\", \"DET\", \"INTJ\", \"N\", \"NUM\", \"PART\", \"PRO\",\n",
    "\"V\", \"V.CVB\", \"V.MSDR\", \"V.PTCP\"],\n",
    "\"Person\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"EXCL\", \"INCL\", \"OBV\", \"PRX\"],\n",
    "\"Polarity\":  [\"NEG\", \"POS\"],\n",
    "\"Politeness\": [\"AVOID\", \"COL\", \"FOREG\", \"FORM\", \"FORM.ELEV\", \"FORM.HUMB\", \"HIGH\", \"HIGH.ELEV\",\n",
    "\"HIGH.SUPR\", \"INFM\", \"LIT\", \"LOW\", \"POL\"],\n",
    "\"Possession\": [\"ALN\", \"NALN\", \"PSSD\", \"PSSPNO+\"],\n",
    "\"Switch-Reference\": [\"CN-R-MN+\", \"DS\", \"DSADV\", \"LOG\", \"OR\", \"SEQMA\", \"SIMMA\", \"SS\", \"SSADV\"],\n",
    "\"Tense\": [\"1DAY\", \"FUT\", \"HOD\", \"IMMED\", \"PRS\", \"PST\", \"RCT\", \"RMT\" ],\n",
    "\"Valency\": [\"DITR\", \"IMPRS\", \"INTR\", \"TR\" ],\n",
    "\"Voice\": [\"ACFOC\", \"ACT\", \"AGFOC\", \"ANTIP\", \"APPL\", \"BFOC\", \"CAUS\", \"CFOC\", \"DIR\", \"IFOC\", \"INV\", \"LFOC\",\n",
    "\"MID\", \"PASS\", \"PFOC\", \"RECP\", \"REFL\"],\n",
    "    \"Language-Specific Features\": [\"LGSPEC\"] #  https://unimorph.github.io/doc/unimorph-schema.pdf\n",
    "}\n",
    "dimension_per_feature = {}\n",
    "for f in features:\n",
    "    print(f)\n",
    "    key = [k for k,values in features_per_dimension.items()  if f in values][0]\n",
    "    dimension_per_feature [f] = key\n",
    "print(\"dimension per feature:\",dimension_per_feature)\n",
    "\n",
    "def intersect (list1,list2):\n",
    "    return [l for l in list1 if l in list2]\n",
    "    \n",
    "features_per_dimension = [(d,intersect(f,features)) for d, f in features_per_dimension.items()]\n",
    "features_per_dimension = [(d,f) for d,f in features_per_dimension if f!=[]]\n",
    "features_per_dimension = dict(features_per_dimension) \n",
    "\n",
    "\n",
    "print(\"features per dimension:\",features_per_dimension)\n",
    "features_per_dimension_old ={\n",
    "    'Animacy': ['ANIM', 'HUM', 'INAN'], \n",
    "    'Aspect': ['IPFV', 'PFV', 'PRF'], \n",
    "    'Case': ['ACC', 'DAT', 'ESS', 'GEN', 'INS', 'NOM', 'VOC'], \n",
    "    'Comparison': ['CMPR', 'SPRL'], \n",
    "    'Finiteness': ['NFIN'], \n",
    "    'Gender+': ['FEM', 'MASC', 'NEUT'], \n",
    "    'Mood': ['COND', 'IMP', 'IND', 'IRR'], \n",
    "    'Number': ['DU', 'PL', 'SG'], \n",
    "    'Parts of Speech': ['ADJ', 'ADV', 'N', 'V', 'V.CVB', 'V.MSDR', 'V.PTCP'], \n",
    "    'Person': ['1', '2', '3'], 'Polarity': ['NEG'], \n",
    "    'Tense': ['FUT', 'PRS', 'PST'], \n",
    "    'Voice': ['ACT', 'PASS'] \n",
    "}\n",
    "features_per_dimension = {\n",
    "    'Animacy': ['ANIM', 'HUM', 'INAN'], \n",
    "     'Aspect': ['IPFV', 'PFV', 'PRF'], \n",
    "     'Case': ['ACC', 'COM', 'DAT', 'ESS', 'FRML', 'GEN', 'INS', 'IN+ALL', 'NOM', 'PRP', 'VOC'], \n",
    "     'Comparison': ['CMPR', 'SPRL'], \n",
    "     'Finiteness': ['NFIN'], \n",
    "     'Gender+': ['FEM', 'MASC', 'NEUT'], \n",
    "     'Mood': ['COND', 'IMP', 'IND', 'IRR', 'POT'], \n",
    "     'Number': ['DU', 'PL', 'SG'], \n",
    "     'Parts of Speech': ['ADJ', 'ADV', 'N', 'V', 'V.CVB', 'V.MSDR', 'V.PTCP'], \n",
    "     'Person': ['1', '2', '3', 'EXCL', 'INCL'], \n",
    "     'Polarity': ['NEG', 'POS'], 'Tense': ['FUT', 'PRS', 'PST'], \n",
    "     'Valency': ['INTR', 'TR'], \n",
    "     'Voice': ['ACT', 'PASS', 'REFL'], \n",
    "     'Language-Specific Features': ['LGSPEC']\n",
    "}\n",
    "new_dimensions = [f for f in features_per_dimension.keys() if f not in features_per_dimension_old.keys()]\n",
    "print(new_dimensions)\n",
    "low_order_hierarchy = {\n",
    "    'Person': 1,\n",
    "    'Number': 2,\n",
    "    'Gender': 3,\n",
    "    'Animacy': 4\n",
    "}\n",
    "\n",
    "low_hierarchy_of_dimension = {\n",
    "    'Parts of Speech': False, #high\n",
    "    'Aspect': False,# high\n",
    "    'Number': True ,# low\n",
    "    'Person': True,# low\n",
    "    'Mood':  False,# high\n",
    "    'Case':  False,# high, (in front of low)\n",
    "    'Gender+':  True,# low\n",
    "    'Tense':  False,# high\n",
    "    'Animacy': True,# low\n",
    "    'Voice':  False,# high\n",
    "    'Finiteness': False, # high\n",
    "    'Comparison': False, # high\n",
    "    'Polarity': False, # high\n",
    "    'Valency': False, # high\n",
    "    'Language-Specific Features': False # high\n",
    "}\n",
    "\n",
    "hierarchy_per_dimension ={\n",
    "    'Animacy': 6,\n",
    "    'Aspect': 1,\n",
    "    'Case': 2,\n",
    "    'Comparison': 1, \n",
    "    'Finiteness': 1, \n",
    "    'Gender+': 5,\n",
    "    'Mood': 1, \n",
    "    'Number': 4, \n",
    "    'Parts of Speech': 0, # first\n",
    "    'Person': 3,\n",
    "    'Polarity': 1, \n",
    "    'Tense': 1, \n",
    "    'Voice':1,\n",
    "    'Valency': 1, # high\n",
    "    'Language-Specific Features': 1 # high\n",
    "}\n",
    "\n",
    "def hierarchical_schema (features):\n",
    "    split_features = features.split(\";\")\n",
    "    POS = split_features[0]\n",
    "\n",
    "    sorted_features = sorted(split_features, key=lambda feature: hierarchy_per_dimension[dimension_per_feature[feature]])\n",
    "    low_hierarchy_features  = filter  (lambda feature:      low_hierarchy_of_dimension[dimension_per_feature[feature]], sorted_features)\n",
    "    high_hierarchy_features = filter  (lambda feature: not  low_hierarchy_of_dimension[dimension_per_feature[feature]], sorted_features)\n",
    "    case = [f for f in sorted_features if dimension_per_feature[f] == \"Case\"]\n",
    "    if POS in ['V', 'V.CVB', 'V.MSDR', 'V.PTCP']:\n",
    "        if low_hierarchy_features == []:\n",
    "            return \";\".join(sorted_features)\n",
    "        else:\n",
    "            return \";\".join(high_hierarchy_features) + \";NOM(\" + \",\".join(low_hierarchy_features) +\")\"\n",
    "    elif POS in ['ADJ', 'ADV', 'N']:\n",
    "        if case == [] or low_hierarchy_features == []:\n",
    "            return \";\".join(sorted_features)\n",
    "        else:\n",
    "            return \";\".join(high_hierarchy_features) + \"(\" + \",\".join(low_hierarchy_features) + \")\"\n",
    "\n",
    "def process_line(x):\n",
    "    x = x.strip()\n",
    "    lemma, target, features = x.split(\"\\t\",2)\n",
    "    features = features.replace(\"\\t\",\";\") # features are in last or two last columns\n",
    "    processed_features = hierarchical_schema(features)\n",
    "    line = lemma + \"\\t\" + processed_features + \"\\t\" + target\n",
    "    return line\n",
    "\n",
    "path_data = \"../data/\"\n",
    "OUT_DIR = \"../../../2023InflectionST/part1/data/\"\n",
    "LANGS = [\"dsb\",\"slk\",\"ces\",\"ote\",\"pol\",\"csb\"]\n",
    "LANGS = []\n",
    "LANGS = [\"mkd\"]\n",
    "datasets = [\"trn\",\"dev\",\"tst\"]\n",
    "for lang in LANGS:\n",
    "    if lang == \"csb\": datasets = [\"tst\"]\n",
    "    else: datasets = [\"trn\",\"dev\",\"tst\"]\n",
    "    for dataset in datasets:\n",
    "        with open (path_data + lang + \".\"+dataset,\"r\") as data_file, open (OUT_DIR + lang + \".\" + dataset,\"w\") as output_file:\n",
    "            data_lines = data_file.readlines()\n",
    "            output_file.write(\"\\n\".join([process_line(line) for line in data_lines]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90eaf3b",
   "metadata": {},
   "source": [
    "# Khaling:\n",
    "LOW_HIERARCHY -> ABS() \n",
    "with ARG -> ERG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f2558ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='../data/klr-add5k-hierarchical.trn' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='../data/klr-add10k-hierarchical.trn' mode='w' encoding='UTF-8'>\n"
     ]
    }
   ],
   "source": [
    "def hierarchical_schema (features):\n",
    "    split_features = features.split(\";\")\n",
    "    POS = split_features[0]\n",
    "    arg_features = [f[3:] for f in split_features if \"ARG\" in f]\n",
    "    normal_features = [f for f in split_features if \"ARG\" not in f]\n",
    "    sorted_features = sorted(normal_features, key=lambda feature: hierarchy_per_dimension[dimension_per_feature[feature]])\n",
    "    low_hierarchy_features  = filter  (lambda feature:      low_hierarchy_of_dimension[dimension_per_feature[feature]], sorted_features)\n",
    "    high_hierarchy_features = filter  (lambda feature: not  low_hierarchy_of_dimension[dimension_per_feature[feature]], sorted_features)\n",
    "    case = [f for f in sorted_features if dimension_per_feature[f] == \"Case\"]\n",
    "    if POS in ['V', 'V.CVB', 'V.MSDR', 'V.PTCP']:\n",
    "        abs_part = \"\" if low_hierarchy_features == [] else \";ABS(\" + \",\".join(low_hierarchy_features) + \")\"\n",
    "        erg_part = \"\" if arg_features == [] else \";ERG(\" + \",\".join(arg_features) + \")\"\n",
    "        normal_part = \";\".join(high_hierarchy_features)\n",
    "        return normal_part + abs_part + erg_part\n",
    "    elif POS in ['ADJ', 'ADV', 'N']:\n",
    "        if case == [] or low_hierarchy_features == []:\n",
    "            return \";\".join(sorted_features)\n",
    "        else:\n",
    "            return \";\".join(high_hierarchy_features) + \"(\" + \",\".join(low_hierarchy_features) + \")\"\n",
    "\n",
    "def process_line(x):\n",
    "    x = x.strip()\n",
    "    lemma, target, features = x.split(\"\\t\",2)\n",
    "    features = features.replace(\"\\t\",\";\") # features are in last or two last columns\n",
    "    processed_features = hierarchical_schema(features)\n",
    "    line = lemma + \"\\t\" + processed_features + \"\\t\" + target\n",
    "    return line\n",
    "\n",
    "path_data = \"../data/\"\n",
    "OUT_DIR = path_data\n",
    "\n",
    "LANGS = [\"klr-add5k\",\"klr-add10k\"]\n",
    "datasets = [\"trn\"]\n",
    "for lang in LANGS:\n",
    "    for dataset in datasets:\n",
    "        with open (path_data + lang + \".\"+dataset,\"r\") as data_file, open (OUT_DIR + lang + \"-hierarchical\"+\".\" + dataset,\"w\") as output_file:\n",
    "            data_lines = data_file.readlines()\n",
    "            output_file.write(\"\\n\".join([process_line(line) for line in data_lines]) + \"\\n\")\n",
    "            print(output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc26063",
   "metadata": {},
   "source": [
    "# Combine the added samples for khaling and sami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4c8ea0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['klr-add5k', 'klr-add10k', 'sme-add5k', 'sme-add10k']\n"
     ]
    }
   ],
   "source": [
    "LANGS = [\"klr\",\"sme\"]\n",
    "adding = [\"-add5k\",\"-add10k\"]\n",
    "datasets = [\".trn\"]\n",
    "print(adding_language)\n",
    "for lang in LANGS:\n",
    "    for dataset in datasets:\n",
    "        for add in adding:\n",
    "            suffix = \"-15ksamples\" if add == \"-add5k\" else \"-20ksamples\"\n",
    "            with open (sigmorphon_path + lang + suffix + dataset, \"w\") as out_file, open (path_data + lang + add + \"-hierarchical\" + dataset,\"r\") as in_added_samples, open (sigmorphon_path + lang + dataset,\"r\") as in_sigmorphon_data:\n",
    "                out_file.write(in_sigmorphon_data.read())\n",
    "                out_file.write(in_added_samples.read())\n",
    "                \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d029f237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_schema (features):\n",
    "    split_features = features.split(\";\")\n",
    "    POS = split_features[0]\n",
    "    sorted_features = sorted(split_features, key=lambda feature: hierarchy_per_dimension[dimension_per_feature[feature]])\n",
    "    low_hierarchy_features  = filter  (lambda feature:      low_hierarchy_of_dimension[dimension_per_feature[feature]], sorted_features)\n",
    "    high_hierarchy_features = filter  (lambda feature: not  low_hierarchy_of_dimension[dimension_per_feature[feature]], sorted_features)\n",
    "    case = [f for f in sorted_features if dimension_per_feature[f] == \"Case\"]\n",
    "    if POS in ['V', 'V.CVB', 'V.MSDR', 'V.PTCP']:\n",
    "        if low_hierarchy_features == []:\n",
    "            return \";\".join(sorted_features)\n",
    "        else:\n",
    "            return \";\".join(high_hierarchy_features) + \";NOM(\" + \",\".join(low_hierarchy_features) +\")\"\n",
    "    elif POS in ['ADJ', 'ADV', 'N']:\n",
    "        if case == [] or low_hierarchy_features == []:\n",
    "            return \";\".join(sorted_features)\n",
    "        else:\n",
    "            return \";\".join(high_hierarchy_features) + \"(\" + \",\".join(low_hierarchy_features) + \")\"\n",
    "\n",
    "def process_line(x):\n",
    "    x = x.strip()\n",
    "    lemma, target, features = x.split(\"\\t\",2)\n",
    "    features = features.replace(\"\\t\",\";\") # features are in last or two last columns\n",
    "    processed_features = hierarchical_schema(features)\n",
    "    line = lemma + \"\\t\" + processed_features + \"\\t\" + target\n",
    "    return line\n",
    "\n",
    "path_data = \"../data/\"\n",
    "OUT_DIR = path_data\n",
    "\n",
    "LANGS = [\"sme-add5k\",\"sme-add10k\"]\n",
    "datasets = [\"trn\"]\n",
    "for lang in LANGS:\n",
    "    for dataset in datasets:\n",
    "        with open (path_data + lang + \".\"+dataset,\"r\") as data_file, open (OUT_DIR + lang + \"-hierarchical\"+\".\" + dataset,\"w\") as output_file:\n",
    "            data_lines = data_file.readlines()\n",
    "            output_file.write(\"\\n\".join([process_line(line) for line in data_lines]) + \"\\n\")\n",
    "            print(output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51866382",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# soruce https://aclanthology.org/P15-2111.pdf\n",
    "# features_per_dimension = {'Parts_of_Speech': ['V', 'N', 'ADJ', 'V.PTCP', 'V.MSDR', 'V.CVB'], # high\n",
    "# 'Aspect': ['IPFV', 'PFV', 'PRF'], # high\n",
    "# 'Number': ['SG', 'PL'], # low\n",
    "# 'Person': ['2', '3', '1'],  # low\n",
    "# 'Mood': ['IRR', 'COND', 'IMP'], # high\n",
    "# 'Case': ['ESS', 'GEN', 'NOM', 'INS', 'DAT', 'VOC', 'ACC'], # high, (in front of low)\n",
    "# 'Gender': ['FEM', 'MASC', 'NEUT'],  # low\n",
    "# 'Tense': ['FUT', 'PRS', 'PST'], # high\n",
    "# 'Animacy': ['HUM', 'ANIM', 'INAN'], # low\n",
    "# 'Voice': ['PASS', 'ACT'], # high\n",
    "# 'Finiteness': ['NFIN']} # high\n",
    "# dimension_per_feature = {\n",
    "#     'V': 'Parts_of_Speech', 'IPFV': 'Aspect', 'SG': 'Number', 'PFV': 'Aspect', 'PRF': 'Aspect', '2': 'Person', '3': 'Person', 'IRR': 'Mood', '1': 'Person', 'N': 'Parts_of_Speech', 'ESS': 'Case', 'PL': 'Number', 'GEN': 'Case', 'NOM': 'Case', 'ADJ': 'Parts_of_Speech', 'INS': 'Case', 'DAT': 'Case', 'VOC': 'Case', 'FEM': 'Gender', 'FUT': 'Tense', 'PRS': 'Tense', 'PST': 'Tense', 'COND': 'Mood', 'IMP': 'Mood', 'ACC': 'Case', 'MASC': 'Gender', 'HUM': 'Animacy', 'NEUT': 'Gender', 'V.PTCP': 'Parts_of_Speech', 'PASS': 'Voice', 'ACT': 'Voice', 'ANIM': 'Animacy', 'V.MSDR': 'Parts_of_Speech', 'INAN': 'Animacy', 'V.CVB': 'Parts_of_Speech', 'NFIN': 'Finiteness'}\n",
    "# low_hierarchy_of_dimension = {\n",
    "#     'Parts_of_Speech': False, #high\n",
    "#     'Aspect': False,# high\n",
    "#     'Number': True ,# low\n",
    "#     'Person': True,# low\n",
    "#     'Mood':  False,# high\n",
    "#     'Case':  False,# high, (in front of low)\n",
    "#     'Gender':  True,# low\n",
    "#     'Tense':  False,# high\n",
    "#     'Animacy': True,# low\n",
    "#     'Voice':  False,# high\n",
    "#     'Finiteness': False, # high\n",
    "# }\n",
    "\n",
    "# low_order_priority = {\n",
    "#     'Person': 1,\n",
    "#     'Number': 2,\n",
    "#     'Gender': 3,\n",
    "#     'Animacy': 4\n",
    "# }\n",
    "\n",
    "# order_priority = {\n",
    "#     'Parts_of_Speech': 0, #high\n",
    "#     'Aspect': 1,# high\n",
    "#     'Number': 4 ,# low\n",
    "#     'Person': 3,# low\n",
    "#     'Mood':  1,# high\n",
    "#     'Case':  2,# high, (in front of low)\n",
    "#     'Gender':  5,# low\n",
    "#     'Tense':  1,# high\n",
    "#     'Animacy': 6,# low\n",
    "#     'Voice':  1,# high\n",
    "#     'Finiteness': 1, # high\n",
    "# }\n",
    "\n",
    "def hierarchical_schema (features):\n",
    "    split_features = features.split(\";\")\n",
    "    POS = split_features[0]\n",
    "    sorted_features = sorted(split_features, key=lambda feature: order_priority[dimension_per_feature[feature]])\n",
    "    low_hierarchy_features  = filter  (lambda feature:      low_hierarchy_of_dimension[dimension_per_feature[feature]], sorted_features)\n",
    "    high_hierarchy_features = filter  (lambda feature: not  low_hierarchy_of_dimension[dimension_per_feature[feature]], sorted_features)\n",
    "    if \"V\" in POS:\n",
    "        return \";\".join(high_hierarchy_features) + \";NOM(\" + \",\".join(low_hierarchy_features) +\")\"\n",
    "    elif POS in (\"N\",\"ADJ\"):\n",
    "        return \";\".join(high_hierarchy_features) + \"(\" + \",\".join(low_hierarchy_features) + \")\"\n",
    "\n",
    "def process_line(x):\n",
    "    x = x.strip()\n",
    "    lemma, target, features = x.split(\"\\t\")\n",
    "    processed_features = hierarchical_schema(features)\n",
    "    line = lemma + \"\\t\" + processed_features + \"\\t\" + target\n",
    "    return line\n",
    "\n",
    "path_data = \"data/\"\n",
    "OUT_DIR = \"../../2023InflectionST/part1/data/\"\n",
    "LANGS = [\"ote\",\"pol\",\"csb\"]\n",
    "datasets = [\"trn\",\"dev\",\"tst\"]\n",
    "for lang in LANGS:\n",
    "    if lang == \"csb\": datasets = [\"tst\"]\n",
    "    else: datasets = [\"trn\",\"dev\",\"tst\"]\n",
    "    for dataset in datasets:\n",
    "        with open (path_data + lang + \".\"+dataset,\"r\") as data_file, open (OUT_DIR + lang + \".\" + dataset,\"w\") as output_file:\n",
    "            data_lines = data_file.readlines()\n",
    "            output_file.write(\"\\n\".join([process_line(line) for line in data_lines]) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
