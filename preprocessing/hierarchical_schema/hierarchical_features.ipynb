{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "804e3956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "more tables than 500 needed for lang: ote  732 792 6366\n",
      "ote needed 1000 tables.\n",
      "more tables than 500 needed for lang: pol  840 936 6816\n",
      "pol needed 1000 tables.\n",
      "number of lemmas in csb testfile:  37\n"
     ]
    }
   ],
   "source": [
    "# split and sample from the paradigms according to the SIGMORPHONâ€“UniMorph 2023 Shared Task 0 Paper\n",
    "def get_lemma(line):\n",
    "    lemma = line.split(\"\\t\")[0]\n",
    "    return lemma\n",
    "\n",
    "from random import sample,shuffle\n",
    "from math import floor\n",
    "from itertools import groupby\n",
    "path_data = \"data/\"\n",
    "LANGS = [\"ote\",\"pol\",\"csb\"]\n",
    "\n",
    "for lang in LANGS:\n",
    "    with open (path_data + lang,\"r\") as data_file, open (path_data + lang + \".trn\",\"w\") as train_file, open (path_data + lang + \".dev\",\"w\") as dev_file,open (path_data + lang + \".tst\",\"w\") as test_file:\n",
    "        data_lines = [line.strip() for line in data_file.readlines()]\n",
    "        tables = [(k, list(g)) for k, g in groupby(data_lines, get_lemma) if k != \"\"]\n",
    "        if lang != \"csb\":\n",
    "            for n in (500,1000,2000,3000):\n",
    "                sampled_tables = sample(tables,n)\n",
    "                size = len(sampled_tables)\n",
    "                datasets = [ sampled_tables[:floor(0.8*size)], sampled_tables[floor(0.8*size):floor(0.9*size)], sampled_tables[floor(0.9*size):] ]\n",
    "\n",
    "                train_forms = [form for lemma,table in datasets[0] for form in table]\n",
    "                dev_forms = [form for lemma,table in datasets[1] for form in table]\n",
    "                test_forms = [form for lemma,table in datasets[2] for form in table]\n",
    "\n",
    "                if len(dev_forms) < 1000 or len(test_forms) < 1000 or len(train_forms) < 10000:\n",
    "                    print(f\"more tables than {n} needed for lang: {lang} \",len(dev_forms),len(test_forms) ,len(train_forms))\n",
    "                else:\n",
    "                    print(f\"{lang} needed {n} tables.\")\n",
    "                    break\n",
    "                    \n",
    "\n",
    "            # sampling and retaining order of the forms\n",
    "            indices_train = sorted(sample(range(len(train_forms)), 10000))\n",
    "            indices_dev =  sorted(sample(range(len(dev_forms)), 1000))\n",
    "            indices_test = sorted( sample(range(len(test_forms)), 1000))\n",
    "\n",
    "            train_samples = [train_forms[i] for i in indices_train]\n",
    "            dev_samples = [dev_forms[i] for i in indices_dev]\n",
    "            test_samples = [test_forms[i] for i in indices_test]\n",
    "            \n",
    "           \n",
    "            train_txt = \"\\n\".join(train_samples) + \"\\n\"\n",
    "            dev_txt = \"\\n\".join(dev_samples) + \"\\n\"\n",
    "            test_txt = \"\\n\".join(test_samples) + \"\\n\"\n",
    "\n",
    "            train_file.write(train_txt)\n",
    "            dev_file.write(dev_txt)\n",
    "            test_file.write(test_txt)\n",
    "        elif lang == \"csb\":\n",
    "            # shuffle lemmas\n",
    "            shuffle(tables) # lemmas should be randomly ordered\n",
    "            test_forms = [form for lemma,table in tables for form in table]\n",
    "            length = len([(k, list(g)) for k, g in groupby(test_forms, get_lemma)])\n",
    "            print(f\"number of lemmas in csb testfile: \",length)\n",
    "            test_txt = \"\\n\".join(test_forms) + \"\\n\"\n",
    "            test_file.write(test_txt)\n",
    "\n",
    "\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4f3f666b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='data/ote.trn' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/ote.dev' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/ote.tst' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/pol.trn' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/pol.dev' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/pol.tst' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/csb.tst' mode='r' encoding='UTF-8'>\n",
      "['V', 'IPFV', 'SG', '2', 'PRS', '3', 'PST', 'PFV', 'PRF', '1', 'IRR', 'N', 'ESS', 'PL', 'INS', 'NOM', 'GEN', 'DAT', 'ACC', 'VOC', 'ADJ', 'FEM', 'NEUT', 'MASC', 'ANIM', 'HUM', 'INAN', 'IMP', 'FUT', 'V.MSDR', 'V.PTCP', 'PASS', 'COND', 'V.CVB', 'ACT', 'NFIN']\n"
     ]
    }
   ],
   "source": [
    "# unique tags    \n",
    "tags = []\n",
    "path_data = \"data/\"\n",
    "LANGS = [\"ote\",\"pol\",\"csb\"]\n",
    "datasets = [\"trn\",\"dev\",\"tst\"]\n",
    "for lang in LANGS:\n",
    "    if lang == \"csb\": datasets = [\"tst\"]\n",
    "    else: datasets = [\"trn\",\"dev\",\"tst\"]\n",
    "    for dataset in datasets:\n",
    "        with open (path_data + lang + \".\"+dataset,\"r\") as data_file:\n",
    "            print(data_file)\n",
    "            data_lines = data_file.readlines()\n",
    "            for line in data_lines:\n",
    "                line = line.strip()\n",
    "                if line == \"\": continue\n",
    "                lemma, target, features = line.split(\"\\t\")\n",
    "                features = features.split(\";\")\n",
    "                for feature in features: \n",
    "                    if feature not in tags: \n",
    "                        tags += [feature]   \n",
    "print(tags)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "51866382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify each feature, sort and rearrange into hierarchical schema\n",
    "# features_in_ote_pol =     ['V', 'IPFV', 'SG', 'PFV', 'PRF', '2', '3', 'IRR', '1', 'N', 'ESS', 'PL', 'GEN', 'NOM', 'ADJ', 'INS', 'DAT', 'VOC', 'FEM', 'FUT', 'PRS', 'PST', 'COND', 'IMP', 'ACC', 'MASC', 'HUM', 'NEUT', 'V.PTCP', 'PASS', 'ACT', 'ANIM', 'V.MSDR', 'INAN', 'V.CVB', 'NFIN']\n",
    "features_in_ote_pol_csb = ['V', 'IPFV', 'SG', '2', 'PRS', '1', 'PST', '3', 'PFV', 'PRF', 'IRR', 'N', 'ESS', 'PL', 'GEN', 'NOM', 'ADJ', 'INS', 'DAT', 'VOC', 'FEM', 'FUT', 'MASC', 'COND', 'IMP', 'HUM', 'ACC', 'NEUT', 'V.PTCP', 'PASS', 'ACT', 'ANIM', 'V.MSDR', 'INAN', 'V.CVB', 'NFIN']      \n",
    "features_in_ote_pol_csb_new = ['V', 'IPFV', 'SG', '1', 'PRS', 'PST', '2', '3', 'PFV', 'PRF', 'IRR', 'MASC', 'N', 'ESS', 'PL', 'INS', 'NOM', 'ACC', 'DAT', 'GEN', 'VOC', 'ADJ', 'FEM', 'NEUT', 'HUM', 'INAN', 'ANIM', 'FUT', 'IMP', 'V.PTCP', 'ACT', 'V.CVB', 'COND', 'PASS', 'V.MSDR', 'NFIN']\n",
    "for f in features_in_ote_pol_csb_new:\n",
    "    if f not in features_in_ote_pol_csb:\n",
    "        print(\"add feature \", f)\n",
    "# soruce https://aclanthology.org/P15-2111.pdf\n",
    "features_per_dimension = {'Parts_of_Speech': ['V', 'N', 'ADJ', 'V.PTCP', 'V.MSDR', 'V.CVB'], # high\n",
    "'Aspect': ['IPFV', 'PFV', 'PRF'], # high\n",
    "'Number': ['SG', 'PL'], # low\n",
    "'Person': ['2', '3', '1'],  # low\n",
    "'Mood': ['IRR', 'COND', 'IMP'], # high\n",
    "'Case': ['ESS', 'GEN', 'NOM', 'INS', 'DAT', 'VOC', 'ACC'], # high, (in front of low)\n",
    "'Gender': ['FEM', 'MASC', 'NEUT'],  # low\n",
    "'Tense': ['FUT', 'PRS', 'PST'], # high\n",
    "'Animacy': ['HUM', 'ANIM', 'INAN'], # low\n",
    "'Voice': ['PASS', 'ACT'], # high\n",
    "'Finiteness': ['NFIN']} # high\n",
    "dimension_per_feature = {\n",
    "    'V': 'Parts_of_Speech', 'IPFV': 'Aspect', 'SG': 'Number', 'PFV': 'Aspect', 'PRF': 'Aspect', '2': 'Person', '3': 'Person', 'IRR': 'Mood', '1': 'Person', 'N': 'Parts_of_Speech', 'ESS': 'Case', 'PL': 'Number', 'GEN': 'Case', 'NOM': 'Case', 'ADJ': 'Parts_of_Speech', 'INS': 'Case', 'DAT': 'Case', 'VOC': 'Case', 'FEM': 'Gender', 'FUT': 'Tense', 'PRS': 'Tense', 'PST': 'Tense', 'COND': 'Mood', 'IMP': 'Mood', 'ACC': 'Case', 'MASC': 'Gender', 'HUM': 'Animacy', 'NEUT': 'Gender', 'V.PTCP': 'Parts_of_Speech', 'PASS': 'Voice', 'ACT': 'Voice', 'ANIM': 'Animacy', 'V.MSDR': 'Parts_of_Speech', 'INAN': 'Animacy', 'V.CVB': 'Parts_of_Speech', 'NFIN': 'Finiteness'}\n",
    "low_hierarchy_of_dimension = {\n",
    "    'Parts_of_Speech': False, #high\n",
    "    'Aspect': False,# high\n",
    "    'Number': True ,# low\n",
    "    'Person': True,# low\n",
    "    'Mood':  False,# high\n",
    "    'Case':  False,# high, (in front of low)\n",
    "    'Gender':  True,# low\n",
    "    'Tense':  False,# high\n",
    "    'Animacy': True,# low\n",
    "    'Voice':  False,# high\n",
    "    'Finiteness': False, # high\n",
    "}\n",
    "\n",
    "low_order_priority = {\n",
    "    'Person': 1,\n",
    "    'Number': 2,\n",
    "    'Gender': 3,\n",
    "    'Animacy': 4\n",
    "}\n",
    "\n",
    "order_priority = {\n",
    "    'Parts_of_Speech': 0, #high\n",
    "    'Aspect': 1,# high\n",
    "    'Number': 4 ,# low\n",
    "    'Person': 3,# low\n",
    "    'Mood':  1,# high\n",
    "    'Case':  2,# high, (in front of low)\n",
    "    'Gender':  5,# low\n",
    "    'Tense':  1,# high\n",
    "    'Animacy': 6,# low\n",
    "    'Voice':  1,# high\n",
    "    'Finiteness': 1, # high\n",
    "}\n",
    "\n",
    "def hierarchical_schema (features):\n",
    "    split_features = features.split(\";\")\n",
    "    POS = split_features[0]\n",
    "    sorted_features = sorted(split_features, key=lambda feature: order_priority[dimension_per_feature[feature]])\n",
    "    low_hierarchy_features  = filter  (lambda feature:      low_hierarchy_of_dimension[dimension_per_feature[feature]], sorted_features)\n",
    "    high_hierarchy_features = filter  (lambda feature: not  low_hierarchy_of_dimension[dimension_per_feature[feature]], sorted_features)\n",
    "    if \"V\" in POS:\n",
    "        return \";\".join(high_hierarchy_features) + \";NOM(\" + \",\".join(low_hierarchy_features) +\")\"\n",
    "    elif POS in (\"N\",\"ADJ\"):\n",
    "        return \";\".join(high_hierarchy_features) + \"(\" + \",\".join(low_hierarchy_features) + \")\"\n",
    "\n",
    "def process_line(x):\n",
    "    x = x.strip()\n",
    "    lemma, target, features = x.split(\"\\t\")\n",
    "    processed_features = hierarchical_schema(features)\n",
    "    line = lemma + \"\\t\" + processed_features + \"\\t\" + target\n",
    "    return line\n",
    "\n",
    "path_data = \"data/\"\n",
    "OUT_DIR = \"../../2023InflectionST/part1/data/\"\n",
    "LANGS = [\"ote\",\"pol\",\"csb\"]\n",
    "datasets = [\"trn\",\"dev\",\"tst\"]\n",
    "for lang in LANGS:\n",
    "    if lang == \"csb\": datasets = [\"tst\"]\n",
    "    else: datasets = [\"trn\",\"dev\",\"tst\"]\n",
    "    for dataset in datasets:\n",
    "        with open (path_data + lang + \".\"+dataset,\"r\") as data_file, open (OUT_DIR + lang + \".\" + dataset,\"w\") as output_file:\n",
    "            data_lines = data_file.readlines()\n",
    "            output_file.write(\"\\n\".join([process_line(line) for line in data_lines]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f71d4825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hierarchical_schema  ote.trn\t   ote_trn.args  pol.dev  pol_dev.args\r\n",
      "ote\t\t     ote.tst\t   ote_tst.args  pol.trn  pol_trn.args\r\n",
      "ote.dev\t\t     ote_dev.args  pol\t\t pol.tst  pol_tst.args\r\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
