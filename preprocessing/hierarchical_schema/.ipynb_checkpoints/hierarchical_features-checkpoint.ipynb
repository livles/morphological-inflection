{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3f666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V;PRF;SG;2;PRS -> V;PRF;NOM(2,SG,PRS)\n",
    "def to_hierarchical_form(f):\n",
    "    global tags\n",
    "    features = f.split(\";\")\n",
    "    word_class = features[0]\n",
    "    if (\"V\" in word_class):\n",
    "        for i in range (len(features)): \n",
    "            if features[i] not in tags: \n",
    "                tags += [features[i]]   \n",
    "                print(f)            \n",
    "            if features[i].isnumeric() or features[i] in (\"SG\",\"PL\"):\n",
    "                # SG;1 -> 1;SG\n",
    "                if i + 1 < len(features) and features[i+1].isnumeric():\n",
    "                    tmp = features[i+1]\n",
    "                    features[i+1] = features[i]\n",
    "                    features[i] = tmp\n",
    "                # V;...;1;SG -> ...;NOM(1,SG)\n",
    "                if len(list(filter(lambda x: x not in (\"PRS\",\"PST\",\"MASC\",\"FEM\",\"NEUT\",\"HUM\") and not x.isnumeric() and x not in (\"SG\",\"PL\"),(features[i:])))): \n",
    "                    print (features[i:])\n",
    "                hierarchical = \"NOM(\"+\",\".join(features[i:])+\")\"\n",
    "                result = \";\".join(features[:i] + [hierarchical])\n",
    "                # print(result)\n",
    "                return result\n",
    "            \n",
    "        return f\n",
    "    elif (word_class in (\"N\",\"ADJ\")):\n",
    "        for i in range (len(features)):\n",
    "            if features[i] not in tags: \n",
    "                tags += [features[i]]   \n",
    "                print(f)  \n",
    "            if features[i] in (\"SG\", \"PL\"):\n",
    "                hierarchical = \"(\"+\",\".join(features[i:])+\")\"\n",
    "                if len(list(filter(lambda x: x not in (\"PRS\",\"PST\",\"MASC\",\"FEM\",\"NEUT\",\"HUM\") and not x.isnumeric() and x not in (\"SG\",\"PL\"),(features[i:])))): \n",
    "                    print (features[i:])\n",
    "                result = \";\".join(features[:i]) + hierarchical\n",
    "                # print(result)\n",
    "                return result\n",
    "        return f\n",
    "    else:\n",
    "        exit(\"invalid word class: \" + word_class)\n",
    "    exit(f)\n",
    "\n",
    "def preprocess(x):\n",
    "    x = x.strip()\n",
    "    lemma, target, features = x.split(\"\\t\")\n",
    "    processed_features = to_hierarchical_form(features)\n",
    "    line = lemma + \"\\t\" + processed_features + \"\\t\" + target\n",
    "    return line\n",
    "tags = []\n",
    "path_data = \"../data/\"\n",
    "LANGS = [\"ote\",\"pol\"]\n",
    "for lang in LANGS:\n",
    "    for dataset in [\"trn\",\"dev\",\"tst\"]:\n",
    "        with open (path_data + lang + \".\"+dataset,\"r\") as data_file, open (path_data + lang + \"_\" + dataset +\".args\",\"w\") as output_file:\n",
    "            data_lines = data_file.readlines()\n",
    "            output_file.write(\"\\n\".join(map(preprocess,data_lines)))\n",
    "print(tags)\n",
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51866382",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/ote.trn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 75\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lang \u001b[38;5;129;01min\u001b[39;00m LANGS:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdev\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtst\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m---> 75\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m data_file, \u001b[38;5;28mopen\u001b[39m (OUT_DIR \u001b[38;5;241m+\u001b[39m lang \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m dataset,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m output_file:\n\u001b[1;32m     76\u001b[0m             data_lines \u001b[38;5;241m=\u001b[39m data_file\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[1;32m     77\u001b[0m             output_file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([process_line(line) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m data_lines]))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/ote.trn'"
     ]
    }
   ],
   "source": [
    "# classify each feature, sort and rearrange into hierarchical schema\n",
    "features_in_ote_pol = ['V', 'IPFV', 'SG', 'PFV', 'PRF', '2', '3', 'IRR', '1', 'N', 'ESS', 'PL', 'GEN', 'NOM', 'ADJ', 'INS', 'DAT', 'VOC', 'FEM', 'FUT', 'PRS', 'PST', 'COND', 'IMP', 'ACC', 'MASC', 'HUM', 'NEUT', 'V.PTCP', 'PASS', 'ACT', 'ANIM', 'V.MSDR', 'INAN', 'V.CVB', 'NFIN']\n",
    "# soruce https://aclanthology.org/P15-2111.pdf\n",
    "features_per_dimension = {'Parts_of_Speech': ['V', 'N', 'ADJ', 'V.PTCP', 'V.MSDR', 'V.CVB'], # high\n",
    "'Aspect': ['IPFV', 'PFV', 'PRF'], # high\n",
    "'Number': ['SG', 'PL'], # low\n",
    "'Person': ['2', '3', '1'],  # low\n",
    "'Mood': ['IRR', 'COND', 'IMP'], # high\n",
    "'Case': ['ESS', 'GEN', 'NOM', 'INS', 'DAT', 'VOC', 'ACC'], # high, (in front of low)\n",
    "'Gender': ['FEM', 'MASC', 'NEUT'],  # low\n",
    "'Tense': ['FUT', 'PRS', 'PST'], # high\n",
    "'Animacy': ['HUM', 'ANIM', 'INAN'], # low\n",
    "'Voice': ['PASS', 'ACT'], # high\n",
    "'Finiteness': ['NFIN']} # high\n",
    "dimension_per_feature = {\n",
    "    'V': 'Parts_of_Speech', 'IPFV': 'Aspect', 'SG': 'Number', 'PFV': 'Aspect', 'PRF': 'Aspect', '2': 'Person', '3': 'Person', 'IRR': 'Mood', '1': 'Person', 'N': 'Parts_of_Speech', 'ESS': 'Case', 'PL': 'Number', 'GEN': 'Case', 'NOM': 'Case', 'ADJ': 'Parts_of_Speech', 'INS': 'Case', 'DAT': 'Case', 'VOC': 'Case', 'FEM': 'Gender', 'FUT': 'Tense', 'PRS': 'Tense', 'PST': 'Tense', 'COND': 'Mood', 'IMP': 'Mood', 'ACC': 'Case', 'MASC': 'Gender', 'HUM': 'Animacy', 'NEUT': 'Gender', 'V.PTCP': 'Parts_of_Speech', 'PASS': 'Voice', 'ACT': 'Voice', 'ANIM': 'Animacy', 'V.MSDR': 'Parts_of_Speech', 'INAN': 'Animacy', 'V.CVB': 'Parts_of_Speech', 'NFIN': 'Finiteness'}\n",
    "low_hierarchy_of_dimension = {\n",
    "    'Parts_of_Speech': False, #high\n",
    "    'Aspect': False,# high\n",
    "    'Number': True ,# low\n",
    "    'Person': True,# low\n",
    "    'Mood':  False,# high\n",
    "    'Case':  False,# high, (in front of low)\n",
    "    'Gender':  True,# low\n",
    "    'Tense':  False,# high\n",
    "    'Animacy': True,# low\n",
    "    'Voice':  False,# high\n",
    "    'Finiteness': False, # high\n",
    "}\n",
    "\n",
    "low_order_priority = {\n",
    "    'Person': 1,\n",
    "    'Number': 2,\n",
    "    'Gender': 3,\n",
    "    'Animacy': 4\n",
    "}\n",
    "\n",
    "order_priority = {\n",
    "    'Parts_of_Speech': 0, #high\n",
    "    'Aspect': 1,# high\n",
    "    'Number': 4 ,# low\n",
    "    'Person': 3,# low\n",
    "    'Mood':  1,# high\n",
    "    'Case':  2,# high, (in front of low)\n",
    "    'Gender':  5,# low\n",
    "    'Tense':  1,# high\n",
    "    'Animacy': 6,# low\n",
    "    'Voice':  1,# high\n",
    "    'Finiteness': 1, # high\n",
    "}\n",
    "\n",
    "def hierarchical_schema (features):\n",
    "    split_features = features.split(\";\")\n",
    "    POS = split_features[0]\n",
    "    sorted_features = sorted(split_features, key=lambda feature: order_priority[dimension_per_feature[feature]])\n",
    "    low_hierarchy_features  = filter  (lambda feature:      low_hierarchy_of_dimension[dimension_per_feature[feature]], sorted_features)\n",
    "    high_hierarchy_features = filter  (lambda feature: not  low_hierarchy_of_dimension[dimension_per_feature[feature]], sorted_features)\n",
    "    if \"V\" in POS:\n",
    "        return \";\".join(high_hierarchy_features) + \";NOM(\" + \",\".join(low_hierarchy_features) +\")\"\n",
    "    elif POS in (\"N\",\"ADJ\"):\n",
    "        return \";\".join(high_hierarchy_features) + \"(\" + \",\".join(low_hierarchy_features) + \")\"\n",
    "\n",
    "def process_line(x):\n",
    "    x = x.strip()\n",
    "    lemma, target, features = x.split(\"\\t\")\n",
    "    processed_features = hierarchical_schema(features)\n",
    "    line = lemma + \"\\t\" + processed_features + \"\\t\" + target\n",
    "    return line\n",
    "\n",
    "path_data = \"./data/\"\n",
    "OUT_DIR = \"../../2023InflectionST/part1/data/\"\n",
    "LANGS = [\"ote\",\"pol\"]\n",
    "for lang in LANGS:\n",
    "    for dataset in [\"trn\",\"dev\",\"tst\"]:\n",
    "        with open (path_data + lang + \".\"+dataset,\"r\") as data_file, open (OUT_DIR + lang + \".\" + dataset,\"w\") as output_file:\n",
    "            data_lines = data_file.readlines()\n",
    "            output_file.write(\"\\n\".join([process_line(line) for line in data_lines]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804e3956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split and sample from the paradigms according to the SIGMORPHONâ€“UniMorph 2023 Shared Task 0 Paper\n",
    "def get_lemma(line):\n",
    "    lemma = line.split(\"\\t\")[0]\n",
    "    return lemma\n",
    "\n",
    "from random import sample\n",
    "from math import floor\n",
    "from itertools import groupby\n",
    "path_data = \"../data/\"\n",
    "LANGS = [\"ote\",\"pol\"]\n",
    "\n",
    "for lang in LANGS:\n",
    "    with open (path_data + lang,\"r\") as data_file, open (path_data + lang + \".trn\",\"w\") as train_file, open (path_data + lang + \".dev\",\"w\") as dev_file,open (path_data + lang + \".tst\",\"w\") as test_file:\n",
    "        data_lines = [line.strip() for line in data_file.readlines()]\n",
    "        tables = [(k, list(g)) for k, g in groupby(data_lines, get_lemma)]   \n",
    "        \n",
    "        sampled_tables = sample(tables,500)\n",
    "        size = len(sampled_tables)\n",
    "        datasets = [ sampled_tables[:floor(0.8*size)], sampled_tables[floor(0.8*size):floor(0.9*size)], sampled_tables[floor(0.9*size):] ]\n",
    "\n",
    "        train_forms = [form for lemma,table in datasets[0] for form in table]\n",
    "        dev_forms = [form for lemma,table in datasets[1] for form in table]\n",
    "        test_forms = [form for lemma,table in datasets[2] for form in table]\n",
    "        \n",
    "        if len(dev_forms) < 1000 or len(test_forms) < 1000 or len(train_forms) < 10000:\n",
    "            exit(\"more tables needed for lang: \" + lang)\n",
    "            \n",
    "        # sampling and retaining order of the forms\n",
    "        indices_train = sorted(sample(range(len(train_forms)), 10000))\n",
    "        indices_dev =  sorted(sample(range(len(dev_forms)), 1000))\n",
    "        indices_test = sorted( sample(range(len(test_forms)), 1000))\n",
    "\n",
    "        train_samples = [train_forms[i] for i in indices_train]\n",
    "        dev_samples = [dev_forms[i] for i in indices_dev]\n",
    "        test_samples = [test_forms[i] for i in indices_test]\n",
    "\n",
    "        train_txt = \"\\n\".join(train_samples) \n",
    "        dev_txt = \"\\n\".join(dev_samples) \n",
    "        test_txt = \"\\n\".join(test_samples) \n",
    "\n",
    "        train_file.write(train_txt)\n",
    "        dev_file.write(dev_txt)\n",
    "        test_file.write(test_txt)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee80a4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hierarchical_schema  ote.trn\t   ote_trn.args  pol.dev  pol_dev.args\n",
      "ote\t\t     ote.tst\t   ote_tst.args  pol.trn  pol_trn.args\n",
      "ote.dev\t\t     ote_dev.args  pol\t\t pol.tst  pol_tst.args\n",
      "hierarchical_schema  ote.trn\t   ote_trn.args  pol.dev  pol_dev.args\n",
      "ote\t\t     ote.tst\t   ote_tst.args  pol.trn  pol_trn.args\n",
      "ote.dev\t\t     ote_dev.args  pol\t\t pol.tst  pol_tst.args\n"
     ]
    }
   ],
   "source": [
    "!ls\n",
    "#%cd data/\n",
    "!ls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
