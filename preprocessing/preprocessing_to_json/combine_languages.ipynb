{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d573e25",
   "metadata": {},
   "source": [
    "# Combine languages with *language code* or language word and convert to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01ca5330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/\n",
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n",
      "<_io.TextIOWrapper name='data/grc-language-code_trn.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/grc-language-code_tst.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/grc-language-code_dev.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/grc-language-word_trn.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/grc-language-word_tst.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/grc-language-word_dev.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/dan-language-code_trn.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/dan-language-code_tst.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/dan-language-code_dev.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/dan-language-word_trn.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/dan-language-word_tst.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/dan-language-word_dev.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/fra-language-code_trn.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/fra-language-code_tst.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/fra-language-code_dev.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/fra-language-word_trn.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/fra-language-word_tst.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/fra-language-word_dev.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/sme-language-code_trn.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/sme-language-code_tst.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/sme-language-code_dev.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/sme-language-word_trn.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/sme-language-word_tst.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/sme-language-word_dev.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/deu-language-code_trn.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/deu-language-code_tst.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/deu-language-code_dev.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/deu-language-word_trn.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/deu-language-word_tst.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/deu-language-word_dev.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/nav-language-code_trn.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/nav-language-code_tst.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/nav-language-code_dev.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/nav-language-word_trn.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/nav-language-word_tst.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/nav-language-word_dev.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/jap-language-code_trn.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/jap-language-code_tst.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/jap-language-code_dev.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/jap-language-word_trn.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/jap-language-word_tst.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/jap-language-word_dev.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/klr-language-code_trn.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/klr-language-code_tst.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/klr-language-code_dev.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/klr-language-word_trn.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/klr-language-word_tst.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/klr-language-word_dev.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/eng-language-code_trn.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/eng-language-code_tst.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/eng-language-code_dev.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/eng-language-word_trn.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/eng-language-word_tst.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/eng-language-word_dev.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/ote-language-code_trn.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/ote-language-code_tst.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/ote-language-code_dev.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/ote-language-word_trn.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/ote-language-word_tst.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/ote-language-word_dev.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/pol-language-code_trn.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/pol-language-code_tst.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/pol-language-code_dev.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/pol-language-word_trn.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/pol-language-word_tst.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/pol-language-word_dev.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/amh-language-code_trn.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/amh-language-code_tst.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/amh-language-code_dev.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/amh-language-word_trn.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/amh-language-word_tst.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/amh-language-word_dev.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/csb-language-code_tst.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/csb-language-word_tst.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/mul-language-code_trn.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/mul-language-code_tst.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/mul-language-code_dev.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/mul-language-word_trn.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/mul-language-word_tst.json' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='data/mul-language-word_dev.json' mode='w' encoding='UTF-8'>\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"../../2023InflectionST/part1/data/\"\n",
    "OUT_DIR = \"data/\"\n",
    "# LANGS = [\"deu\",\"eng\"]\n",
    "# TARGET_LANG = \"deu_eng\"\n",
    "# LANGS = [\"grc\",\"dan\",\"fra\",\"sme\",\"deu\",\"nav\",\"jap\",\"klr\",\"eng\"]\n",
    "# TARGET_LANG = \"mul\"\n",
    "LANGS_UNIQUE = [\"grc\",\"dan\",\"fra\",\"sme\",\"deu\",\"nav\",\"jap\",\"klr\",\"eng\"]+[\"ote\",\"pol\",\"amh\",\"csb\"]\n",
    "#TARGET_LANG = \"mul_2\"\n",
    "LANG_WORD_OF_CODE = {\"grc\":\"Ancient Greek\",\n",
    "                  \"dan\": \"Danish\",\n",
    "                  \"fra\":\"French\",\n",
    "                  \"sme\":\"SÃ¡mi\",\n",
    "                  \"deu\":\"German\",\n",
    "                  \"nav\":\"Navajo\",\n",
    "                  \"jap\":\"Japanese\",\n",
    "                  \"klr\":\"Khaling\",\n",
    "                  \"eng\":\"English\",\n",
    "                  \"ote\":\"Mezquital Otomi\",\n",
    "                  \"pol\":\"Polish\",\n",
    "                  \"amh\":\"Amharic\",\n",
    "                  \"csb\":\"Kashubian\"}\n",
    "\n",
    "TARGET_LANGS = [\"grc\",\"dan\",\"fra\",\"sme\",\"deu\",\"nav\",\"jap\",\"klr\",\"eng\",\"ote\",\"pol\",\"amh\",\"csb\",\"mul\"]\n",
    "DATASET_COMPLETE = [\"trn\",\"tst\",\"dev\"]\n",
    "LANGUAGE_CODES = [True,False]\n",
    "print(OUT_DIR)\n",
    "for target_lang in TARGET_LANGS:\n",
    "    target_lang_initials = target_lang\n",
    "    for lang_code in LANGUAGE_CODES:\n",
    "        if lang_code: \n",
    "            target_lang = target_lang_initials  + \"-language-code\"\n",
    "        else: \n",
    "            target_lang = target_lang_initials  + \"-language-word\"\n",
    "            \n",
    "        for dataset in DATASET_COMPLETE:\n",
    "            if dataset != \"tst\" and  \"csb\" in target_lang: continue # csb only as test data\n",
    "                \n",
    "            OUT=OUT_DIR + target_lang  + \"_\" + dataset + \".json\"\n",
    "            with open (OUT,\"w\") as out_file:\n",
    "                print(out_file)\n",
    "                if target_lang in [\"mul-language-code\",\"mul-language-word\"]: LANGS = LANGS_UNIQUE\n",
    "                else: LANGS = [target_lang_initials]\n",
    "                for lang in LANGS:\n",
    "                    if dataset != \"tst\" and lang == \"csb\": continue # csb only as test data\n",
    "                        \n",
    "                    with open(DATA_PATH + lang + \".\" + dataset,\"r\") as in_file:\n",
    "                        for line in in_file:\n",
    "                            line = line.strip()\n",
    "                            lemma, features, target = line.split(\"\\t\")\n",
    "                            if lang_code: context = lang + \": \"\n",
    "                            else: context = LANG_WORD_OF_CODE[lang]+ \": \"\n",
    "                            json_line = f'{{\"input\": \"{context}Inflect {lemma} | {features}\",\"target\": \"{target}\"}}'\n",
    "                            out_file.write(json_line + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67f83cb-9c9e-4ab5-a066-887219fd3205",
   "metadata": {},
   "source": [
    "# Sub-sampling for the multilingual model\n",
    "In order to have the same amount of training data compared with the monolingual models, we sample 10000 forms for the training of 5lang-sample.\n",
    "+ we sample from 5 languages 2000 forms and 200 form for training and dev set respectively.\n",
    "+ we sample 100, 200, 400, 600 tables from the given SIGMORPHON-UniMorph 2023 shared task data\n",
    "+ we sample from the tables then the forms, keeping the forms per lemma together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7bd99a4-487d-4f8f-a0f7-5446bca8e439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nav needed 100.0 tables.\n",
      "more tables than 100.0 needed for lang: deu  118 127 965\n",
      "more tables than 200.0 needed for lang: deu  247 247 1924\n",
      "deu needed 400.0 tables.\n",
      "more tables than 100.0 needed for lang: klr  245 240 1930\n",
      "klr needed 200.0 tables.\n",
      "more tables than 100.0 needed for lang: amh  145 145 953\n",
      "more tables than 200.0 needed for lang: amh  238 237 1864\n",
      "amh needed 400.0 tables.\n",
      "more tables than 100.0 needed for lang: grc  173 243 2036\n",
      "grc needed 200.0 tables.\n"
     ]
    }
   ],
   "source": [
    "from random import sample,shuffle\n",
    "from math import floor\n",
    "from itertools import groupby\n",
    "\n",
    "def get_lemma(line):\n",
    "    lemma = line.split(\"\\t\")[0]\n",
    "    return lemma\n",
    "\n",
    "def to_json_format (line, prefix):\n",
    "    lemma, features, target = line.split(\"\\t\")\n",
    "    json_line = f'{{\"input\": \"{prefix}Inflect {lemma} | {features}\",\"target\": \"{target}\"}}'\n",
    "    return json_line\n",
    "    \n",
    "LANGS = {\"nav\":\"Navajo\", \"deu\":\"German\", \"klr\":\"Khaling\", \"amh\":\"Amharic\", \"grc\":\"Ancient Greek\"}\n",
    "PATH_DATA = \"../../2023InflectionST/part1/data/\"\n",
    "OUTDIR = \"data/\"\n",
    "SPLIT_DATASETS = [\".tst\",\".trn\",\".dev\"]\n",
    "target_lang = \"5lang-sample\" # sample from the 5 languages with language name as prefix\n",
    "with open (OUTDIR + target_lang + \"_trn.json\", \"w\") as trn_out, open (OUTDIR + target_lang + \"_tst.json\", \"w\") as tst_out, open (OUTDIR + target_lang + \"_dev.json\", \"w\") as dev_out:\n",
    "    for lang in LANGS.keys():\n",
    "            with open (PATH_DATA + lang + \".trn\",\"r\") as train_data, open (PATH_DATA + lang + \".dev\",\"r\") as dev_data,open (PATH_DATA + lang + \".tst\",\"r\") as test_data:\n",
    "                data_lines = [line.strip() for line in train_data.readlines() + dev_data.readlines() + test_data.readlines()]\n",
    "\n",
    "                tables = [(k, list(g)) for k, g in groupby(data_lines, get_lemma) if k != \"\"]\n",
    "                \n",
    "                for n in (500/len(LANGS) ,1000/len(LANGS) ,2000/len(LANGS) ,3000/len(LANGS) ):\n",
    "                    sampled_tables = sample(tables,int(n))\n",
    "                    size = len(sampled_tables)\n",
    "                    datasets = [ sampled_tables[:floor(0.8*size)], sampled_tables[floor(0.8*size):floor(0.9*size)], sampled_tables[floor(0.9*size):] ]\n",
    "    \n",
    "                    train_forms = [form for lemma,table in datasets[0] for form in table]\n",
    "                    dev_forms = [form for lemma,table in datasets[1] for form in table]\n",
    "                    test_forms = [form for lemma,table in datasets[2] for form in table]\n",
    "    \n",
    "                    if len(dev_forms) < 1000/len(LANGS) or len(test_forms) < 1000/len(LANGS) or len(train_forms) < 10000/len(LANGS):\n",
    "                        print(f\"more tables than {n} needed for lang: {lang} \",len(dev_forms),len(test_forms) ,len(train_forms))\n",
    "                    else:\n",
    "                        print(f\"{lang} needed {n} tables.\")\n",
    "                        break\n",
    "                        \n",
    "    \n",
    "                # sampling and retaining order of the forms\n",
    "                indices_train = sorted(sample(range(len(train_forms)), int(10000/len(LANGS)) ))\n",
    "                indices_dev =  sorted(sample(range(len(dev_forms)), int(1000/len(LANGS)) ))\n",
    "                indices_test = sorted( sample(range(len(test_forms)), int(1000/len(LANGS)) ))\n",
    "    \n",
    "                train_samples = [to_json_format(train_forms[i],LANGS[lang]+\": \") for i in indices_train]\n",
    "                dev_samples = [to_json_format(dev_forms[i],LANGS[lang]+\": \") for i in indices_dev]\n",
    "                test_samples = [to_json_format(test_forms[i],LANGS[lang]+\": \") for i in indices_test]\n",
    "\n",
    "                train_txt = \"\\n\".join(train_samples) + \"\\n\"\n",
    "                dev_txt = \"\\n\".join(dev_samples) + \"\\n\"\n",
    "                test_txt = \"\\n\".join(test_samples) + \"\\n\"\n",
    "    \n",
    "                trn_out.write(train_txt)\n",
    "                dev_out.write(dev_txt)\n",
    "                tst_out.write(test_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c49a0378-5018-4eb4-b90a-4928fc8ada08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dss/dsshome1/03/ge87wod2/morphological-inflection/preprocessing/preprocessing_to_json\n",
      "afb.covered.dev  eng.trn\t\tita.dev\t\t pol.dev\n",
      "afb.covered.tst  eng.tst\t\tita.out\t\t pol.out\n",
      "afb.dev\t\t fin.covered.dev\tita.trn\t\t pol.trn\n",
      "afb.out\t\t fin.covered.tst\tita.tst\t\t pol.tst\n",
      "afb.trn\t\t fin.dev\t\tjap.covered.dev  rus.covered.dev\n",
      "afb.tst\t\t fin.out\t\tjap.covered.tst  rus.covered.tst\n",
      "amh.covered.dev  fin.trn\t\tjap.dev\t\t rus.dev\n",
      "amh.covered.tst  fin.tst\t\tjap.out\t\t rus.out\n",
      "amh.dev\t\t fra.covered.dev\tjap.trn\t\t rus.trn\n",
      "amh.out\t\t fra.covered.tst\tjap.tst\t\t rus.tst\n",
      "amh.trn\t\t fra.dev\t\tkat.covered.dev  san.covered.dev\n",
      "amh.tst\t\t fra.out\t\tkat.covered.tst  san.covered.tst\n",
      "arz.covered.dev  fra.trn\t\tkat.dev\t\t san.dev\n",
      "arz.covered.tst  fra.tst\t\tkat.out\t\t san.out\n",
      "arz.dev\t\t grc.covered.dev\tkat.trn\t\t san.trn\n",
      "arz.out\t\t grc.covered.tst\tkat.tst\t\t san.tst\n",
      "arz.trn\t\t grc.dev\t\tklr.covered.dev  sme.covered.dev\n",
      "arz.tst\t\t grc.out\t\tklr.covered.tst  sme.covered.tst\n",
      "bel.covered.dev  grc.trn\t\tklr.dev\t\t sme.dev\n",
      "bel.covered.tst  grc.tst\t\tklr.out\t\t sme.out\n",
      "bel.dev\t\t heb.covered.dev\tklr.trn\t\t sme.trn\n",
      "bel.out\t\t heb.covered.tst\tklr.tst\t\t sme.tst\n",
      "bel.trn\t\t heb.dev\t\tmkd.covered.dev  spa.covered.dev\n",
      "bel.tst\t\t heb.out\t\tmkd.covered.tst  spa.covered.tst\n",
      "csb.out\t\t heb.trn\t\tmkd.dev\t\t spa.dev\n",
      "csb.tst\t\t heb.tst\t\tmkd.out\t\t spa.out\n",
      "dan.covered.dev  heb_unvoc.covered.dev\tmkd.trn\t\t spa.trn\n",
      "dan.covered.tst  heb_unvoc.covered.tst\tmkd.tst\t\t spa.tst\n",
      "dan.dev\t\t heb_unvoc.dev\t\tmul.dev\t\t sqi.covered.dev\n",
      "dan.out\t\t heb_unvoc.out\t\tmul.out\t\t sqi.covered.tst\n",
      "dan.trn\t\t heb_unvoc.trn\t\tmul.trn\t\t sqi.dev\n",
      "dan.tst\t\t heb_unvoc.tst\t\tmul.tst\t\t sqi.out\n",
      "deu.covered.dev  hun.covered.dev\tmul_2.dev\t sqi.trn\n",
      "deu.covered.tst  hun.covered.tst\tmul_2.out\t sqi.tst\n",
      "deu.dev\t\t hun.dev\t\tmul_2.trn\t swa.covered.dev\n",
      "deu.out\t\t hun.out\t\tmul_2.tst\t swa.covered.tst\n",
      "deu.trn\t\t hun.trn\t\tnav.covered.dev  swa.dev\n",
      "deu.tst\t\t hun.tst\t\tnav.covered.tst  swa.out\n",
      "deu_eng.dev\t hye.covered.dev\tnav.dev\t\t swa.trn\n",
      "deu_eng.out\t hye.covered.tst\tnav.out\t\t swa.tst\n",
      "deu_eng.trn\t hye.dev\t\tnav.trn\t\t tur.covered.dev\n",
      "deu_eng.tst\t hye.out\t\tnav.tst\t\t tur.covered.tst\n",
      "eng.covered.dev  hye.trn\t\tote.dev\t\t tur.dev\n",
      "eng.covered.tst  hye.tst\t\tote.out\t\t tur.out\n",
      "eng.dev\t\t ita.covered.dev\tote.trn\t\t tur.trn\n",
      "eng.out\t\t ita.covered.tst\tote.tst\t\t tur.tst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n",
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ~/morphological-inflection/preprocessing/preprocessing_to_json/\n",
    "!ls ../../2023InflectionST/part1/data/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
